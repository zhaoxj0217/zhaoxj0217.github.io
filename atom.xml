<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhaoxj0217_blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhaoxj0217.github.io/"/>
  <updated>2017-12-06T08:35:40.172Z</updated>
  <id>http://zhaoxj0217.github.io/</id>
  
  <author>
    <name>zhaoxj0217</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>java常用算法</title>
    <link href="http://zhaoxj0217.github.io/2017/12/06/java_algorithm/"/>
    <id>http://zhaoxj0217.github.io/2017/12/06/java_algorithm/</id>
    <published>2017-12-06T08:14:06.904Z</published>
    <updated>2017-12-06T08:35:40.172Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>排序和查找算法  </li>
<li>单链表  </li>
<li>二叉树  </li>
<li>队列和栈  </li>
<li>字符串  </li>
<li>数组  </li>
<li>其它算法  </li>
</ul>
<p>在排序中，90%的概率会考察快速排序算法，一些简单的时候，会要求二分查找算法</p>
<p>快速排序：是一种分区交换排序算法。采用分治策略对两个子序列再分别进行快速排序，是一种递归算法。<br>算法描述：在数据序列中选择一个元素作为基准值，每趟从数据序列的两端开始交替进行，将小于基准值的元素交换到序列前端，将大于基准值的元素交换到序列后端，介于两者之间的位置则成为了基准值的最终位置。同时，序列被划分成两个子序列，再分别对两个子序列进行快速排序，直到子序列的长度为1，则完成排序。</p>
<p>举例：假设要排序的数组是a[6],长度为7，首先任意选取一个数据（通常选用第一个数据）作为关键数据，然后将所有比它的数都放到它前面，所有比它大的数都放到它后面，这个过程称为一躺快速排序。一躺快速排序的算法是：  </p>
<p>设置两个变量i，j，排序开始的时候i=0；j=6； </p>
<ul>
<li>以第一个数组元素作为关键数据，赋值给key，即key=a[0]；  </li>
<li>从j开始向前搜索，即由后开始向前搜索（j–），找到第一个小于key的值，两者交换；  </li>
<li>从i开始向后搜索，即由前开始向后搜索（i++），找到第一个大于key的值，两者交换；  </li>
<li>重复第3、4步，直到i=j；此时将key赋值给a[i]；  </li>
</ul>
<p>例如：待排序的数组a的值分别是：（初始关键数据key=49）</p>
<p><img src="http://images.gitbook.cn/07827ec0-bd77-11e7-a4d0-01a65989f9d2" alt=""></p>
<p>此时完成了一趟循环，将49赋值给a[3]，数据分为三组，分别为{27,38,13}{49}{76,96,65}，利用递归，分别对第一组和第三组进行排序，则可得到一个有序序列，这就是快速排序算法。<br>快速排序代码如下</p>
<pre><code>public void quickSort(int[] num, int left, int right) { 
    if (num == null) return; //如果左边大于右边，则return，这里是递归的终点，需要写在前面。 
    if (left &gt;= right) { return; } 
    int i = left; 
    int j = right;
     int temp = num[i]; //此处开始进入遍历循环 
    while (i &lt; j) { //从右往左循环 
        while (i &lt; j &amp;&amp; num[j] &gt;= temp) {//如果num[j]大于temp值，则pass，比较下一个 
            j--; } 
        num[i] = num[j]; 
           while (i &lt; j &amp;&amp; num[i] &lt;= temp) { 
            i++; } 
        num[j] = num[i]; 
        num[i] = temp; // 此处不可遗漏，将基准值插入到指定位置 
    } 
    quickSort(num, left, i - 1); 
    quickSort(num, i + 1, right); 
}
</code></pre><p><strong>二分查找又称折半查找</strong>，优点是比较次数少，查找速度快，平均性能好；其缺点是要求待查表为有序表，且插入删除困难。因此，折半查找方法适用于不经常变动而查找频繁的有序列表。  </p>
<p>步骤：首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。</p>
<p>算法前提：必须采用顺序存储结构；必须按关键字大小有序排列。  </p>
<p>实现方式：包含递归实现和非递归实现两种方式。二分查找的递归代码实现如下：  </p>
<pre><code>private  int halfSearch(int[] a,int left,int right,int target) {  
 int mid=(left+right)/2;  
 int midValue=a[mid];  
 if(left&lt;=right){  
     if(midValue&gt;target){  
         return halfSearch(a, left, mid-1, target);  
     }else if(midValue&lt;target) {  
         return halfSearch(a, mid+1, right, target);  
     }else {  
         return mid;  
     }  
}  
return -1;  
}
</code></pre><p>非递归实现代码如下：</p>
<pre><code>private  int halfSearch(int[] a,int target){  
int i=0;  
int j=a.length-1;  
while(i&lt;=j){  
    int mid=(i+j)/2;  
    int midValue=a[mid];  
    if(midValue&gt;target){  
        j=mid-1;  
    }else if(midValue&lt;target){  
        i=mid+1;  
    }else {  
        return mid;  
    }  
}  
return -1;  
}
</code></pre><p><a href="http://blog.csdn.net/qq_25827845/article/details/74058248" target="_blank" rel="external">更多排序查找算法</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;排序和查找算法  &lt;/li&gt;
&lt;li&gt;单链表  &lt;/li&gt;
&lt;li&gt;二叉树  &lt;/li&gt;
&lt;li&gt;队列和栈  &lt;/li&gt;
&lt;li&gt;字符串  &lt;/li&gt;
&lt;li&gt;数组  &lt;/li&gt;
&lt;li&gt;其它算法  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在排序中，90%的概率会考察快速
    
    </summary>
    
      <category term="algorithm" scheme="http://zhaoxj0217.github.io/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://zhaoxj0217.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>线上问题处理-OOM</title>
    <link href="http://zhaoxj0217.github.io/2017/12/06/product_handle_OOM/"/>
    <id>http://zhaoxj0217.github.io/2017/12/06/product_handle_OOM/</id>
    <published>2017-12-06T07:39:36.980Z</published>
    <updated>2017-12-06T07:47:46.566Z</updated>
    
    <content type="html"><![CDATA[<p>某服务器上部署了Java服务一枚，出现了OutOfMemoryError，请问有可能是什么原因，问题应该如何定位？</p>
<p><strong>解决思路</strong><br>Java服务OOM，最常见的原因为：<br>•    有可能是内存分配确实过小，而正常业务使用了大量内存<br>•    某一个对象被频繁申请，却没有释放，内存不断泄漏，导致内存耗尽<br>•    某一个资源被频繁申请，系统资源耗尽，例如：不断创建线程，不断发起网络连接  </p>
<p>更具体的，可以使用以下的一些工具逐一排查。</p>
<p><strong>一、确认是不是内存本身就分配过小</strong><br>方法：jmap -heap 10765<br><img src="https://i.loli.net/2017/12/06/5a279f80b43d8.png" alt="jmap_linux.png"><br>如上图，可以查看新生代，老生代堆内存的分配大小以及使用情况，看是否本身分配过小</p>
<p><strong>二、找到最耗内存的对象</strong><br>方法：jmap -histo:live 10765 | more<br><img src="https://i.loli.net/2017/12/06/5a279fdca45d2.png" alt="jmap_histo_linux.png"><br>如上图，输入命令后，会以表格的形式显示存活对象的信息，并按照所占内存大小排序：<br>•    <strong>实例数</strong><br>•    <strong>所占内存大小</strong><br>•    <strong>类名</strong><br>是不是很直观？对于实例数较多，占用内存大小较多的实例/类，相关的代码就要针对性review了。  </p>
<p>上图中占内存最多的对象是RingBufferLogEvent，共占用内存18M，属于正常使用范围。  </p>
<p>如果发现某类对象占用内存很大（例如几个G），很可能是类对象创建太多，且一直未释放。例如：<br>•    申请完资源后，未调用close()或dispose()释放资源<br>•    消费者消费速度慢（或停止消费了），而生产者不断往队列中投递任务，导致队列中任务累积过多  </p>
<p><strong>三、确认是否是资源耗尽</strong></p>
<p>工具：<br>•    pstree<br>•    netstat<br>查看进程创建的线程数，以及网络连接数，如果资源耗尽，也可能出现OOM。  </p>
<p>这里介绍另一种方法，通过<br>•    /proc/${PID}/fd<br>•    /proc/${PID}/task<br>可以分别查看句柄详情和线程数。  </p>
<p>例如，某一台线上服务器的sshd进程PID是9339，查看<br>•    ll /proc/9339/fd<br>•    ll /proc/9339/task  </p>
<p><img src="https://i.loli.net/2017/12/06/5a27a09940d82.png" alt="proc_linux.png"></p>
<p>如上图，sshd共占用了四个句柄<br>•    0 -&gt; 标准输入<br>•    1 -&gt; 标准输出<br>•    2 -&gt; 标准错误输出<br>•    3 -&gt; socket（容易想到是监听端口）  </p>
<p>sshd只有一个主线程PID为9339，并没有多线程。  </p>
<p>所以，只要<br>•    ll /proc/${PID}/fd | wc -l<br>•    ll /proc/${PID}/task | wc -l （效果等同pstree -p | wc -l）<br>就能知道进程打开的句柄数和线程数。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;某服务器上部署了Java服务一枚，出现了OutOfMemoryError，请问有可能是什么原因，问题应该如何定位？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决思路&lt;/strong&gt;&lt;br&gt;Java服务OOM，最常见的原因为：&lt;br&gt;•    有可能是内存分配确实过小，而正常业务使用
    
    </summary>
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/categories/accident-handling/"/>
    
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/tags/accident-handling/"/>
    
  </entry>
  
  <entry>
    <title>线上问题处理-CPU过高</title>
    <link href="http://zhaoxj0217.github.io/2017/12/06/product_handle_high_CPU/"/>
    <id>http://zhaoxj0217.github.io/2017/12/06/product_handle_high_CPU/</id>
    <published>2017-12-06T07:28:00.474Z</published>
    <updated>2017-12-06T07:37:49.157Z</updated>
    
    <content type="html"><![CDATA[<p>某服务器上部署了若干tomcat实例，即若干垂直切分的Java站点服务，以及若干Java微服务，突然收到运维的CPU异常告警。<br>问：如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？</p>
<p><strong>步骤一、找到最耗CPU的进程</strong></p>
<p>工具：top<br>方法：<br>•    执行top -c ，显示进程运行信息列表<br>•    键入P (大写p)，进程按照CPU使用率排序<br><img src="https://i.loli.net/2017/12/06/5a279cce5c796.png" alt="top_linux.png"><br>如上图，最耗CPU的进程PID为10765</p>
<p><strong>步骤二：找到最耗CPU的线程</strong><br>工具：top<br>方法：<br>•    top -Hp 10765 ，显示一个进程的线程运行信息列表<br>•    键入P (大写p)，线程按照CPU使用率排序<br><img src="https://i.loli.net/2017/12/06/5a279d2c284d8.png" alt="top_hp_linux.png"><br>如上图，进程10765内，最耗CPU的线程PID为10804</p>
<p><strong>步骤三：将线程PID转化为16进制</strong><br>工具：printf<br>方法：printf “%x\n” 10804<br><img src="https://i.loli.net/2017/12/06/5a279db425e22.png" alt="printf_16_linux.png"><br>如上图，10804对应的16进制是0x2a34，当然，这一步可以用计算器。</p>
<p>之所以要转化为16进制，是因为堆栈里，线程id是用16进制表示的。</p>
<p><strong>步骤四：查看堆栈，找到线程在干嘛</strong><br>工具：pstack/jstack/grep<br>方法：jstack 10765 | grep ‘0x2a34’ -C5 –color<br>•    打印进程堆栈<br>•    通过线程id，过滤得到线程堆栈<br><img src="https://i.loli.net/2017/12/06/5a279e337516e.png" alt="jstack_grep_linux.png"><br>如上图，找到了耗CPU高的线程对应的线程名称“AsyncLogger-1”，以及看到了该线程正在执行代码的堆栈。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;某服务器上部署了若干tomcat实例，即若干垂直切分的Java站点服务，以及若干Java微服务，突然收到运维的CPU异常告警。&lt;br&gt;问：如何定位是哪个服务进程导致CPU过载，哪个线程导致CPU过载，哪段代码导致CPU过载？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤一、找到最耗C
    
    </summary>
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/categories/accident-handling/"/>
    
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/tags/accident-handling/"/>
    
  </entry>
  
  <entry>
    <title>线上问题处理-服务假死</title>
    <link href="http://zhaoxj0217.github.io/2017/12/06/product_handle_service_suspended/"/>
    <id>http://zhaoxj0217.github.io/2017/12/06/product_handle_service_suspended/</id>
    <published>2017-12-06T02:39:08.328Z</published>
    <updated>2017-12-06T07:22:11.066Z</updated>
    
    <content type="html"><![CDATA[<p>1.服务假死<br>现象一般如下，比如：服务器没有后续日志输出，或者接口只有进的日志，没有出的日志。</p>
<p>操作步骤：<br> 1）联系运维，在出问题的服务器上输入以下命令，保存当前stack日志：  </p>
<pre><code>jstack -l  服务pid &gt; stack.log
</code></pre><p> 2）打开保存的日志，分析主线当前状态，<font color="#DC143C">搜索”main”</font>，比如下面日志标识主线程当前在sleep：  </p>
<pre><code>&quot;main&quot; #1 prio=5 os_prio=0 tid=0x00007faf9c00a000 nid=0x7661 waiting on condition [0x00007fafa36a8000]
java.lang.Thread.State: TIMED_WAITING (sleeping)
at java.lang.Thread.sleep(Native Method)
</code></pre><p> 3）打开保存的日志，分析可能存在的死锁，<font color="#DC143C">搜索”deadlock”或者”BLOCKED”</font>， 比如下面日志，表示可能存在死锁：  </p>
<pre><code> &quot;IdleRemover&quot; daemon prio=10 tid=0x00007f6b2c148800 nid=0x11d7 waiting on condition [0x00007f6b222e1000]
   java.lang.Thread.State: TIMED_WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
......
 Locked ownable synchronizers://正常情况下，线程不会有：Locked ownable synchronizers信息
  - &lt;0x0000000765df4068&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)
</code></pre><p>4）如果线程信息没有异常，需要确认FGC是否存在问题？比如： 频繁full gc 或者单词full gc时间较长。</p>
<pre><code>jstat  -gcutil pid 2500 70
[root@iZbp1aehttqhrozii8qa5iZ Python-3.6.3]# jstat -gcutil 10567 2500 70
S0  S1   E   O  MCCS  YGC YGCT FGC FGCT GCT 
0.00 54.17 19.32 17.82 97.59 95.77 1777 11.596 4 0.898 12.494
0.00 54.17 19.58 17.82 97.59 95.77 1777 11.596 4 0.898 12.494
0.00 54.17 19.82 17.82 97.59 95.77 1777 11.596 4 0.898 12.494
0.00 54.17 20.16 17.82 97.59 95.77 1777 11.596 4 0.898 12.494
</code></pre><p>S0 — Heap上的 Survivor space 0 区已使用空间的百分比<br>S1 — Heap上的 Survivor space 1 区已使用空间的百分比<br>E — Heap上的 Eden space 区已使用空间的百分比<br>O — Heap上的 Old space 区已使用空间的百分比<br>P — Perm space 区已使用空间的百分比<br>YGC — 从应用程序启动到采样时发生 Young GC 的次数<br>YGCT– 从应用程序启动到采样时 Young GC 所用的时间（单位秒）<br>FGC — 从应用程序启动到采样时发生 Full GC 的次数<br>FGCT– 从应用程序启动到采样时 Full GC 所用的时间（单位秒）<br>GCT — 从应用程序启动到采样时用于垃圾回收的总时间（单位秒）  </p>
<p><strong>jstack Dump 日志文件中的线程状态</strong><br>dump 文件里，值得关注的线程状态有：  </p>
<ul>
<li><ol>
<li>死锁，<font color="#DC143C"> Deadlock（重点关注）</font>   </li>
</ol>
</li>
<li><ol>
<li>执行中，Runnable     </li>
</ol>
</li>
<li><ol>
<li>等待资源，<font color="#DC143C"> Waiting on condition（重点关注</font>）   </li>
</ol>
</li>
<li><ol>
<li>等待获取监视器，<font color="#DC143C"> Waiting on monitor entry（重点关注）</font>  </li>
</ol>
</li>
<li><ol>
<li>暂停，Suspended  </li>
</ol>
</li>
<li><ol>
<li>对象等待中，Object.wait() 或 TIMED_WAITING  </li>
</ol>
</li>
<li><ol>
<li>阻塞，<font color="#DC143C"> Blocked（重点关注）</font>    </li>
</ol>
</li>
<li><ol>
<li>停止，Parked  </li>
</ol>
</li>
</ul>
<p><strong>Dump文件中的线程状态含义及注意事项</strong></p>
<p>•    <strong>Deadlock</strong>：死锁线程，一般指多个线程调用间，进入相互资源占用，导致一直等待无法释放的情况。</p>
<p>•    <strong>Runnable</strong>：一般指该线程正在执行状态中，该线程占用了资源，正在处理某个请求，有可能正在传递SQL到数据库执行，有可能在对某个文件操作，有可能进行数据类型等转换。</p>
<p>•    <strong>Waiting on condition</strong>：等待资源，或等待某个条件的发生。具体原因需结合 stacktrace来分析。 </p>
<ul>
<li>如果堆栈信息明确是应用代码，则证明该线程正在等待资源。一般是大量读取某资源，且该资源采用了资源锁的情况下，线程进入等待状态，等待资源的读取。  </li>
<li>又或者，正在等待其他线程的执行等。  </li>
<li>如果发现有大量的线程都在处在 Wait on condition，从线程 stack看，正等待网络读写，这可能是一个网络瓶颈的征兆。因为网络阻塞导致线程无法执行。<br>–    一种情况是网络非常忙，几乎消耗了所有的带宽，仍然有大量数据等待网络读写；<br>–     另一种情况也可能是网络空闲，但由于路由等问题，导致包无法正常的到达。  </li>
<li>另外一种出现 Wait on condition的常见情况是该线程在 sleep，等待 sleep的时间到了时候，将被唤醒。  </li>
</ul>
<p>•    <strong>Blocked</strong>：线程阻塞，是指当前线程执行过程中，所需要的资源长时间等待却一直未能获取到，被容器的线程管理器标识为阻塞状态，可以理解为等待资源超时的线程。   </p>
<p>•    <strong>Waiting for monitor entry 和 in Object.wait()</strong>：Monitor是 Java中用以实现线程之间的互斥与协作的主要手段，它可以看成是对象或者 Class的锁。每一个对象都有，也仅有一个 monitor。从下图1中可以看出，每个 Monitor在某个时刻，只能被一个线程拥有，该线程就是 “Active Thread”，而其它线程都是 “Waiting Thread”，分别在两个队列 “ Entry Set”和 “Wait Set”里面等候。在 “Entry Set”中等待的线程状态是 “Waiting for monitor entry”，而在 “Wait Set”中等待的线程状态是 “in Object.wait()”。</p>
<p><img src="https://i.loli.net/2017/12/06/5a278e71877cf.png" alt="java_monitor.png"></p>
<p><strong>示例一：Waiting to lock 和 Blocked</strong><br>实例如下：  </p>
<pre><code>&quot;RMI TCP Connection(267865)-172.16.5.25&quot; daemon prio=10 tid=0x00007fd508371000 nid=0x55ae waiting for monitor entry [0x00007fd4f8684000]
   java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.log4j.Category.callAppenders(Category.java:201)
- waiting to lock &lt;0x00000000acf4d0c0&gt; (a org.apache.log4j.Logger)
at org.apache.log4j.Category.forcedLog(Category.java:388)
at org.apache.log4j.Category.log(Category.java:853)
at org.apache.commons.logging.impl.Log4JLogger.warn(Log4JLogger.java:234)
at com.tuan.core.common.lang.cache.remote.SpyMemcachedClient.get(SpyMemcachedClient.java:110)
</code></pre><p>1）线程状态是 <strong>Blocked</strong>，阻塞状态。说明线程等待资源超时！<br>2）“ <font color="#DC143C"> waiting to lock <0x00000000acf4d0c0></0x00000000acf4d0c0></font>”指，线程在等待给这个 0x00000000acf4d0c0 地址上锁（英文可描述为：trying to obtain  0x00000000acf4d0c0 lock）。<br>3）在 dump 日志里查找字符串 0x00000000acf4d0c0，发现有大量线程都在等待给这个地址上锁。如果能在日志里找到谁获得了这个锁（如locked &lt; 0x00000000acf4d0c0 &gt;），就可以顺藤摸瓜了。<br>4）“<font color="#DC143C">waiting for monitor entry</font>”说明此线程通过 synchronized(obj) {……} 申请进入了临界区，从而进入了上图中的“Entry Set”队列，但该 obj 对应的 monitor 被其他线程拥有，所以本线程在 Entry Set 队列中等待。<br>5）第一行里，”<font color="#DC143C">RMI TCP Connection(267865)-172.16.5.25</font>“是 Thread Name 。tid指Java Thread id。nid指native线程的id。prio是线程优先级。[0x00007fd4f8684000]是线程栈起始地址。  </p>
<p><strong>示例二：Waiting on condition 和 TIMED_WAITING</strong><br>实例如下：  </p>
<pre><code>&quot;RMI TCP Connection(idle)&quot; daemon prio=10 tid=0x00007fd50834e800 nid=0x56b2 waiting on condition [0x00007fd4f1a59000]
   java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  &lt;0x00000000acd84de8&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:945)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:662)
</code></pre><p>1）“<font color="#DC143C">TIMED_WAITING (parking)</font>”中的 timed_waiting 指等待状态，但这里指定了时间，到达指定的时间后自动退出等待状态；parking指线程处于挂起中。</p>
<p>2）“<font color="#DC143C">waiting on condition</font>”需要与堆栈中的“<font color="#DC143C">parking to wait for  <0x00000000acd84de8> (a java.util.concurrent.SynchronousQueue$TransferStack)</0x00000000acd84de8></font>”结合来看。首先，本线程肯定是在等待某个条件的发生，来把自己唤醒。其次，SynchronousQueue 并不是一个队列，只是线程之间移交信息的机制，当我们把一个元素放入到 SynchronousQueue 中时必须有另一个线程正在等待接受移交的任务，因此这就是本线程在等待的条件。</p>
<p><strong>示例三：in Obejct.wait() 和 TIMED_WAITING</strong><br>实例如下：  </p>
<pre><code>&quot;RMI RenewClean-[172.16.5.19:28475]&quot; daemon prio=10 tid=0x0000000041428800 nid=0xb09 in Object.wait() [0x00007f34f4bd0000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on &lt;0x00000000aa672478&gt; (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
- locked &lt;0x00000000aa672478&gt; (a java.lang.ref.ReferenceQueue$Lock)
at sun.rmi.transport.DGCClient$EndpointEntry$RenewCleanThread.run(DGCClient.java:516)
at java.lang.Thread.run(Thread.java:662)
</code></pre><p>1）“<font color="#DC143C">TIMED_WAITING (on object monitor)</font>”，对于本例而言，是因为本线程调用了 java.lang.Object.wait(long timeout) 而进入等待状态。<br>2）“Wait Set”中等待的线程状态就是“ <font color="#DC143C">in Object.wait() </font>”。当线程获得了 Monitor，进入了临界区之后，如果发现线程继续运行的条件没有满足，它则调用对象（一般就是被 synchronized 的对象）的 wait() 方法，放弃了 Monitor，进入 “Wait Set”队列。只有当别的线程在该对象上调用了 notify() 或者 notifyAll() ，“ Wait Set”队列中线程才得到机会去竞争，但是只有一个线程获得对象的 Monitor，恢复到运行态。<br>3）RMI RenewClean 是 DGCClient 的一部分。DGC 指的是 Distributed GC，即分布式垃圾回收。<br>4）请注意，是先 <font color="#DC143C">locked <0x00000000aa672478></0x00000000aa672478></font>，后 <font color="#DC143C">waiting on <0x00000000aa672478></0x00000000aa672478></font>，之所以先锁再等同一个对象，请看下面它的代码实现：  </p>
<pre><code>static private class  Lock { };
private Lock lock = new Lock();
public Reference&lt;? extends T&gt; remove(long timeout)
{
    synchronized (lock) {
        Reference&lt;? extends T&gt; r = reallyPoll();
        if (r != null) return r;
        for (;;) {
            lock.wait(timeout);
            r = reallyPoll();
               ……
   }
}
</code></pre><p>即，线程的执行中，先用 synchronized 获得了这个对象的 Monitor（对应于 <font color="#DC143C"> locked <0x00000000aa672478></0x00000000aa672478></font> ）；当执行到 lock.wait(timeout);，线程就放弃了 Monitor 的所有权，进入“Wait Set”队列（对应于  <font color="#DC143C">waiting on <0x00000000aa672478> </0x00000000aa672478></font>）。<br>5）从堆栈信息看，是正在清理 remote references to remote objects ，引用的租约到了，分布式垃圾回收在逐一清理呢。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1.服务假死&lt;br&gt;现象一般如下，比如：服务器没有后续日志输出，或者接口只有进的日志，没有出的日志。&lt;/p&gt;
&lt;p&gt;操作步骤：&lt;br&gt; 1）联系运维，在出问题的服务器上输入以下命令，保存当前stack日志：  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jstack -l  服务pid
    
    </summary>
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/categories/accident-handling/"/>
    
    
      <category term="accident handling" scheme="http://zhaoxj0217.github.io/tags/accident-handling/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络LeNet-5</title>
    <link href="http://zhaoxj0217.github.io/2017/11/29/LeNet-5/"/>
    <id>http://zhaoxj0217.github.io/2017/11/29/LeNet-5/</id>
    <published>2017-11-29T09:51:40.681Z</published>
    <updated>2017-12-04T07:09:57.431Z</updated>
    
    <content type="html"><![CDATA[<p>非线性特征工程–&gt; 非线性分类器<br>Xgboost<br>RF<br>GBDT<br>adaboost</p>
<p>线性特征工程–&gt;  线性分类器<br>SVM<br>LR<br>MLP</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;非线性特征工程–&amp;gt; 非线性分类器&lt;br&gt;Xgboost&lt;br&gt;RF&lt;br&gt;GBDT&lt;br&gt;adaboost&lt;/p&gt;
&lt;p&gt;线性特征工程–&amp;gt;  线性分类器&lt;br&gt;SVM&lt;br&gt;LR&lt;br&gt;MLP&lt;/p&gt;

    
    </summary>
    
      <category term="深度学习" scheme="http://zhaoxj0217.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LeNet-5" scheme="http://zhaoxj0217.github.io/tags/LeNet-5/"/>
    
      <category term="深度学习" scheme="http://zhaoxj0217.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树之ID3算法，C4.5</title>
    <link href="http://zhaoxj0217.github.io/2017/11/29/Decision_Tree_Learning/"/>
    <id>http://zhaoxj0217.github.io/2017/11/29/Decision_Tree_Learning/</id>
    <published>2017-11-29T09:45:36.657Z</published>
    <updated>2017-12-05T08:19:30.739Z</updated>
    
    <content type="html"><![CDATA[<p>信息增益（香农熵/熵）<br>熵是表示随机变量不确定性的度量<br>条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性<br>信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度<br>定义:特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H（D）与特征A给定条件下D的经验条件熵H（D|A）之差，及g(D,A)=H(D)-H(D|A)<br>信息增益比：特征A对训练数据集D的信息增益比gr(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比。 </p>
<p>ID3算法生成决策树<br>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。具体的方法是：从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的额特征作为结点的特征，由该特征的不同取值简历子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。直到得到一个决策树。</p>
<p>ID3算法<br>基于奥卡姆剃刀原理的,即用尽量用较少的东西做更多的事<br>是一种自顶向下的贪心策略<br>输入：训练数据集D，特征集A，阈值ε<br>输出：决策树T<br>（1）若D中所有实例属于同一类Ck,则T为单结点树，并将类Ck作该结点的类标记，返回T；<br>（2）若A=Ø，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；<br>（3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征Ag；<br>（4）如果Ag的信息增益小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；<br>（5）否则，对Ag的每一个可能值ai，依Ag=ai将D分割为若干非空子集Di,将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T<br>（6）对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步1~步5，得到子树Ti,返回Ti</p>
<p>贷款申请样本数据表</p>
<p>| ID | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |<br>| 1  | 青年 | 否     | 否         | 一般     | 否 |<br>| 2  | 青年 | 否     | 否         | 好       | 否 |<br>| 3  | 青年 | 是     | 否         | 好       | 是 |<br>| 4  | 青年 | 是     | 是         | 一般     | 是 |<br>| 5  | 青年 | 否     | 否         | 一般     | 否 |<br>| 6  | 中年 | 否     | 否         | 一般     | 否 |<br>| 7  | 中年 | 否     | 否         | 好       | 否 |<br>| 8  | 中年 | 是     | 是         | 好       | 是 |<br>| 9  | 中年 | 否     | 是         | 非常好   | 是 |<br>| 10 | 中年 | 否     | 是         | 非常好   | 是 |<br>| 11 | 老年 | 否     | 是         | 非常好   | 是 |<br>| 12 | 老年 | 否     | 是         | 好       | 是 |<br>| 13 | 老年 | 是     | 否         | 好       | 是 |<br>| 14 | 老年 | 是     | 否         | 非常好    | 是 |<br>| 15 | 老年 | 否     | 否         | 一般      | 否 |  </p>
<p>首先计算经验熵H（D）<br><img src="https://i.loli.net/2017/12/04/5a251868957b2.png" alt="HD.PNG"><br>再分别以A1,A2,A3,A4表示年龄，有工作，有自己的房子和信贷情况4个特征，计算信息增益<br>年龄<br><img src="https://i.loli.net/2017/12/04/5a251918f40e6.png" alt="gDA1.PNG">   </p>
<p>有工作<br><img src="https://i.loli.net/2017/12/04/5a25198a32c9c.png" alt="gDA2.PNG"></p>
<p>有自己的房子<br><img src="https://i.loli.net/2017/12/04/5a251af3d52bb.png" alt="gDA3.PNG">  </p>
<p>信贷情况 略<br>g(D|A4) = 0.971-0.608 =0.363  </p>
<p>最后，由于特征A3的信息增益最大，所以选择特征A3作为最优特征<br>以A3为根结点的特征，将训练数据集D划分为两个子集D1（A3取值为是）和D2（A3取值为否）由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为是<br>对D2从A1年龄，A2有工作，和A4信贷情况中选择新的特征</p>
<p><img src="https://i.loli.net/2017/12/04/5a251f3e86411.png" alt="gD2A.PNG">  </p>
<p>选择信息增益最大的特征A2作为节点的特征，由于A2有两个可能取值，从这一节点引出两个子节点，一个对应“是”，包含3个样本，它们属于同一类，所以这是一个叶结点，另一个是对应“否”，包含6个样本，他们也属于同一类，所以这也是一个叶结点，类标记为“否”<br>最终生成决策树  </p>
<p><img src="https://i.loli.net/2017/12/04/5a2522a745f9a.png" alt="decision_final.PNG"></p>
<p>ID3算法存在的缺点</p>
<p>（1）ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。<br>（2）ID3算法只能对描述属性为离散型属性的数据集构造决策树。 </p>
<p>C4.5的生成算法<br>C4.5算法与ID3算法相似，C4.5在生成的过程中，用信息增益率来选择特征</p>
<p>使用信息增益的问题：假设每个属性中没中类别都只有一个样本，这样的属性信息熵就等于零，根据信息增益就无法选择出有效分类特征</p>
<p>计算属性分裂信息度量<br>用分裂信息度量来考量某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息成为属性的内在信息，信息增益率= 信息增益/内在信息，会导致属性的重要性随着内在信息的增大而减小（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取他）</p>
<p>上述A1，A2，A3，A4信息增益率计算<br><img src="https://i.loli.net/2017/12/05/5a264e1eb0bc9.jpg" alt="IMG_1260.JPG"></p>
<p>输入：训练数据集D，特征集A，阈值ε<br>输出：决策树T<br>（1）若D中所有实例属于同一类Ck,则T为单结点树，并将类Ck作该结点的类标记，返回T；<br>（2）若A=Ø，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；<br>（3）否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag；<br>（4）如果Ag的信息增益比小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；<br>（5）否则，对Ag的每一个可能值ai，依Ag=ai将D分割为若干非空子集Di,将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T<br>（6）对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步1~步5，得到子树Ti,返回Ti  </p>
<p>决策树的剪枝<br>在决策树学习中将已生成的树进行简化的过程称为剪枝。剪枝一般分两种方法：先剪枝和后剪枝。  </p>
<p>先剪枝<br> 先剪枝方法中通过提前停止树的构造（比如决定在某个节点不再分裂或划分训练元组的子集）而对树剪枝。一旦停止，这个节点就变成树叶，该树叶可能取它持有的子集最频繁的类作为自己的类。先剪枝有很多方法:  </p>
<ul>
<li>（1）当决策树达到一定的高度就停止决策树的生长；</li>
<li>（2）到达此节点的实例具有相同的特征向量，而不必一定属于同一类，也可以停止生长</li>
<li>（3）到达此节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数据量比较小的特殊情况  </li>
<li>（4）计算每次扩展对系统性能的增益，如果小于某个阈值就可以让它停止生长。先剪枝有个缺点就是视野效果问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步扩展又能满足要求。这样会过早停止决策树的生长。  </li>
</ul>
<p>后剪枝<br>更常用的方法是后剪枝，它由完全成长的树剪去子树而形成。通过删除节点的分枝并用树叶来替换它。树叶一般用子树中最频繁的类别来标记。后剪枝一般有两种方法：   </p>
<ul>
<li><p>第一种方法，也是最简单的方法，<strong>称之为基于误判的剪枝</strong>。这个思路很直接，完全的决策树不是过度拟合么，我再搞一个测试数据集来纠正它。对于完全决策树中的每一个非叶子节点的子树，我们尝试着把它替换成一个叶子节点，该叶子节点的类别我们用子树所覆盖训练样本中存在最多的那个类来代替，这样就产生了一个简化决策树，然后比较这两个决策树在测试数据集中的表现，如果简化决策树在测试数据集中的错误比较少，并且该子树里面没有包含另外一个具有类似特性的子树（所谓类似的特性，指的就是把子树替换成叶子节点后，其测试数据集误判率降低的特性），那么该子树就可以替换成叶子节点。该算法以bottom-up的方式遍历所有的子树，直至没有任何子树可以替换使得测试数据集的表现得以改进时，算法就可以终止。</p>
</li>
<li><p>第一种方法很直接，但是需要一个额外的测试数据集，能不能不要这个额外的数据集呢？为了解决这个问题，于是就提出了<strong>悲观剪枝</strong>。悲观剪枝就是递归得估算每个内部节点所覆盖样本节点的误判率。剪枝后该内部节点会变成一个叶子节点，该叶子节点的类别为原内部节点的最优叶子节点所决定。然后比较剪枝前后该节点的错误率来决定是否进行剪枝。该方法和前面提到的第一种方法思路是一致的，不同之处在于如何估计剪枝前分类树内部节点的错误率。</p>
</li>
</ul>
<p><em>对于连续数据的处理</em><br>离散化处理：将连续型的属性变量进行离散化处理，形成决策树的训练集，分三步： </p>
<ul>
<li><ol>
<li>把需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序  </li>
</ol>
</li>
<li><ol>
<li>假设该属性对应的不同的属性值一共有N个，那么总共有N-1个可能的候选分割阈值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点  </li>
</ol>
</li>
<li><ol>
<li>用信息增益率选择最佳划分  </li>
</ol>
</li>
</ul>
<p><em>对于缺失值的处理</em><br>缺失值：在某些情况下，可供使用的数据可能缺少某些属性的值。例如(X, y)是样本集S中的一个训练实例，X=(F1_v,F2_v, …Fn_v)。但是其属性Fi的值Fi_v未知。<br>处理策略：  </p>
<ul>
<li><ol>
<li>处理缺少属性值的一种策略是赋给它结点t所对应的训练实例中该属性的最常见值  </li>
</ol>
</li>
<li><ol>
<li>另外一种更复杂的策略是为Fi的每个可能值赋予一个概率。例如，给定一个布尔属性Fi，如果结点t包含6个已知Fi_v=1和4个Fi_v=0的实例，那么Fi_v=1的概率是0.6，而Fi_v=0的概率是0.4。于是，实例x的60%被分配到Fi_v=1的分支，40%被分配到另一个分支。这些片断样例（fractional examples）的目的是计算信息增益，另外，如果有第二个缺少值的属性必须被测试，这些样例可以在后继的树分支中被进一步细分。(C4.5中使用)  </li>
</ol>
</li>
<li><ol>
<li>简单处理策略就是丢弃这些样本  </li>
</ol>
</li>
</ul>
<p>C4.5算法的核心思想是ID3算法，是ID3算法的改进：  </p>
<ul>
<li>用信息增益率来选择属性，克服了用信息增益来选择属性时变相选择取值多的属性的不足；  </li>
<li>在树的构造过程中进行剪枝；  </li>
<li>能处理非离散化数据；  </li>
<li>能处理不完整数据。  </li>
</ul>
<p>优点：<br>产生的分类规则易于理解，准确率高。  </p>
<p>缺点：<br>在构造过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；<br>C4.5算法只适合于能够驻留内存的数据集，当训练集大得无法在内存容纳时，程序无法运行</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;信息增益（香农熵/熵）&lt;br&gt;熵是表示随机变量不确定性的度量&lt;br&gt;条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性&lt;br&gt;信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度&lt;br&gt;定义:特征A对训练数据集D的信息增益g(D,A),定义为集合D
    
    </summary>
    
      <category term="decision tree,Machine Learning,ID3,C4.5" scheme="http://zhaoxj0217.github.io/categories/decision-tree-Machine-Learning-ID3-C4-5/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="decision tree" scheme="http://zhaoxj0217.github.io/tags/decision-tree/"/>
    
  </entry>
  
  <entry>
    <title>验证码图片预处理—python</title>
    <link href="http://zhaoxj0217.github.io/2017/11/16/catptch_pretreatment/"/>
    <id>http://zhaoxj0217.github.io/2017/11/16/catptch_pretreatment/</id>
    <published>2017-11-16T08:57:13.289Z</published>
    <updated>2017-11-16T09:48:17.443Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要讨论下验证码识别之前一般都会做的验证码的预处理</p>
<p>验证码的预处理一般有一下几个步骤  </p>
<ul>
<li>图片去噪（去干扰点及干扰线）  </li>
<li>将彩色图片二值化为黑白图片  </li>
<li>图片字符切割    </li>
<li>图片尺寸归一化  </li>
<li>图片字符标记</li>
</ul>
<p>以上的步骤可以根据实际情况做先后顺序的调整</p>
<p><strong>图片去噪</strong>  </p>
<ul>
<li>去噪点待补充  参考<a href="http://blog.csdn.net/ysc6688/article/details/50772382" target="_blank" rel="external">http://blog.csdn.net/ysc6688/article/details/50772382</a>  </li>
<li>去干扰线待补充  参考<a href="http://blog.csdn.net/ysc6688/article/details/44540623" target="_blank" rel="external">http://blog.csdn.net/ysc6688/article/details/44540623</a>  </li>
</ul>
<p><strong>二值化图片</strong><br>主要步骤如下：<br>将RGB彩图转为灰度图<br>将灰度图按照设定阈值转化为二值图</p>
<p>一、灰度化<br>灰度化应用很广，而且也比较简单。灰度图就是将白与黑中间的颜色等分为若干等级，绝大多数位256阶。在RGB模型种，黑色（R=G=B=0）与白色（R=G=B=255），那么256阶的灰度划分就是R=G=B=i，其中i取0到255.<br>一般图像的颜色数据矩阵默认是3通道的，也就是RGB模型，所以每个pixel都有3个分量，分别代表r，g和b的值。因此将三个分量值都改为同一个灰度值，图片就实现灰度化。<br>灰度化的方法一般有以下几种：   </p>
<ol>
<li>分量法<br>在rgb三个分量种按照需求选取一个分量作为灰度值   </li>
<li>最大值<br>选取rgb的最大值作为该pixel的灰度值   </li>
<li>平均值<br>g[i,j] = (r[i,j] + g[i,j] + b[i,j]) / 3,取rgb的平均值作为灰度值</li>
<li>加权变换<br>由于人眼对绿色的敏感最高，对蓝色敏感最低，因此，按下式对RGB三分量进行加权平均能得到较合理的灰度图像。    L = R <em> 299/1000 + G </em> 587/1000 + B <em> 114/1000  
</em>PIL 包中的Image.convert(‘L’)  就是采用的这种计算方式*</li>
</ol>
<p>灰度后的图片如下</p>
<p><img src="https://i.loli.net/2017/11/16/5a0d523089f32.png" alt="9639_L.png"></p>
<p>二、二值化<br>二值化故名思议，就是整个图像所有像素只有两个值可以选择，一个是黑（灰度为0），一个是白（灰度为255）。二值化的好处就是将图片上的有用信息和无用信息区分开来，比如二值化之后的验证码图片，验证码像素为黑色，背景和干扰点为白色，这样后面对验证码像素处理的时候就会很方便。<br>常见的二值化方法为固定阀值和自适应阀值，固定阀值就是制定一个固定的数值作为分界点，大于这个阀值的像素就设为255，小于该阀值就设为0，这种方法简单粗暴，但是效果不一定好.另外就是自适应阀值，每次根据图片的灰度情况找合适的阀值。自适应阀值的方法有很多，这里采用了一种类似K均值的方法，就是先选择一个值作为阀值，统计大于这个阀值的所有像素的灰度平均值和小于这个阀值的所有像素的灰度平均值，再求这两个值的平均值作为新的阀值。重复上面的计算，直到每次更新阀值后，大于该阀值和小于该阀值的像素数目不变为止。<br>python代码如下：</p>
<pre><code>#获取一个图片的自适应阈值
def adaptiveThreshold(image):
      img_raw =  np.array(image)
    width, height = image.size
    ucThre = 0;  
    ucThre_new = 127;#该值初始不等于ucThre即可  

    while ucThre != ucThre_new:
        nBack_count = 0
        nData_count = 0   
        nBack_sum = 0
        nData_sum = 0  
        for j in  range(height):
            for i in range(width):
                nValue = img_raw[j][i]  
                if nValue &gt; ucThre_new:  
                    nBack_sum += nValue
                    nBack_count+=1
                else:  
                    nData_sum += nValue
                    nData_count+=1  
        nBack_sum = nBack_sum / nBack_count
        nData_sum = nData_sum / nData_count
        ucThre = ucThre_new
        ucThre_new = (nBack_sum + nData_sum) / 2                
    return math.ceil(ucThre_new)
</code></pre><p>定义二值函数：  </p>
<pre><code>def get_bin_table(threshold=230):
    table = []
    for i in range(256):
        if i &lt; threshold:
            table.append(0)
        else:
            table.append(1)

    return table
</code></pre><p>完整将一个图片二值化的过程如下：  </p>
<pre><code>image = Image.open(img_path)
imgry = image.convert(&apos;L&apos;)  # 转化为灰度图    
table = get_bin_table(adaptiveThreshold(imgry))
out = imgry.point(table, &apos;1&apos;)
</code></pre><p>经过上述转化后的二值化图片如下<br><img src="https://i.loli.net/2017/11/16/5a0d52533f8f5.png" alt="9639_bina.png"></p>
<p><strong>图片字符切割</strong>  </p>
<p>前面经过各种去除噪点、干扰线，验证码图片现在已经只有两个部分，白色背景及黑色字符。为了字符的识别，这里需要将图片上的字符一个一个“扣”下来，得到单个的字符，接下来再进行识别。<br>字符分割可以说是图像验证码识别最关键的一步，因为分割的正确与否直接关系到最后的结果，如果4个字符分割成了3个，即便后面的识别算法识别率达到100%，结果也是错的。当然，前面预处理如果做得够好，干扰因素能够有效的去除，而没有影响到字符的像素，那么分割来讲要容易得多。反过来，如果前面的干扰因素都没有去除掉，那么分割出来的可能就不是字符了。<br>字符的粘连是分割的难点，这一点也可以作为验证码安全系数的标准，如果验证码上的几个字符完全是分开的，那么可以保证字符分割成功率百分之百，这样验证码破解的难度就降低了很多<br>上述处理过的简单示例就是一个没有黏连的验证码<br>可以有2中简单的处理方式   </p>
<p>一、洪水填充法   </p>
<p>洪水填充法主要思路还是连通域的思想。对于相互之间没有粘连的字符验证码，直接对图片进行扫描，遇到一个黑的pixel就对其进行填充，所有与其连通的字符都被标记出来，因此一个独立的字符就能够找到了。这个方法优点是效率高，时间复杂度是O（N），N为像素的个数；而且不用考虑图片的大小、相邻字符间隔以及字符在图片中得位置等其他任何因素，任何验证码图片只要字符相互是独立的，不需要对其他任何阀值做预处理，直接就操作；用这种方法分割正确率非常高，几乎不会出现分割错误的情况。但是缺点也很致命：那就是字符之间必须完全隔离，没有粘连的部分，否则会将两个字符误认为一个字符。同时若有不相连的字母如i,j也不太好处理</p>
<p>二、X像素投影法</p>
<p>对于粘连的字符，也并非没有方法分割。一个方法就是将两个粘连的验证码一刀切开，从哪里切？当然是从粘连的薄弱的地方切。前面提到过图片的像素就像一个二维的矩阵，对每一个x值，统计所有x值为这个值的pixel中黑色的数目，直观来讲就是统计每一条竖线上黑色点的数目。显而易见的是，如果这一条线为背景，那么这一条线肯定都是白色的，那么黑色点的数目为0，如果一条竖线经过字符，那么这条竖线上的黑色点数目肯定不少。<br>对于完全独立的两个字符之间，肯定有黑色点数目为0的竖线，但是如果粘连，那么不会有黑色点数为0的竖线存在，但是字符粘连最薄弱的地方一定是黑色点数目最少的那条竖线，因此切就要从这个地方切。<br>在代码的实现的过程中，可以先从左到右扫描一遍，统计投影到每个X值的黑色点的数目，然后设定一个阀值范围，这个阀值大概就是一个字符的宽度。从左到右，先找到第一个x黑色点投影不为0的x值，然后在这个x值加上大概一个字符宽度的大小找到x投影数目最小的x值，这两个x值分割出来就是一个字符了。<br>这个方法的特点就是能够分割粘连的字符，但是缺点就是容易分割不干净，可能会出现分割错误的情况，另外就是需要提供相应的阀值。如果验证码字体倾斜比较厉害，就不可以使用，比如<br><img src="https://i.loli.net/2017/11/16/5a0d526c41e22.jpg" alt="20150323195933323.jpg"></p>
<pre><code># 图像处理-X像素投影法,0像素切割
def splitByPixel(image, text):
# img_raw = np.array(image)
    w, h = image.size
    splitCol = getSplit(image)

    if len(splitCol) != len(text):
        print(&quot;切割错误！&quot;, text)
        return &quot;&quot;

    for i in range(len(text)):
        cropImg = image.crop((splitCol[i][0], 0, splitCol[i][1], h))
        cropImg = resizeImge(cropImg, int(w / int(len(text))), h)  # 切割的图片尺寸不一，统一尺寸
        char = text[i]
        save_path = &quot;./knnImage/letters/&quot; + str(char) + &quot;/&quot; + str(uuid.uuid4()) + &quot;.png&quot;
        cropImg.save(save_path)  # 保存切割下来的图片


# 获取切割列0像素切割
def getSplit(image):
    img_raw = np.array(image)
    w, h = image.size
    allCol = []
    for i in range(w):
        allCol.append(i)
    charCol = []
    for i in range(w):
        for j in range(h):
            if img_raw[j][i] != True:
                charCol.append(i)
                break
    # 找到0像素的列
    zeroCol = [i for i in allCol if i not in charCol]
    # 从0像素列中找出切割列
    splitCol = []
    col = zeroCol[1]
    for vi, vv in enumerate(zeroCol):
        tmparr = []
        if vv &lt; col + 4:
            col += 1
        else:
            if vi &gt; 0:
                tmparr.append(zeroCol[vi - 1])
            else:
                tmparr.append(zeroCol[0])
            tmparr.append(vv)
            splitCol.append(tmparr)
            col = vv

    return splitCol
</code></pre><p><strong>图片尺寸归一化</strong><br>在图片切割的时候，就将切割完的图片统一尺寸</p>
<pre><code>def resizeImge(image,rew,reh):
    img_raw = np.array(image)
    w, h = image.size
    top = 0
    bottom =h
    try:
        for i in range (h):
            for j in range(w):
                if img_raw[i][j] != True:
                    top = i - 1
                    raise Getoutofloop()
    except Getoutofloop:
        pass

    try:
        for i in range(h):
            for j in range(w):
                if img_raw[h-1-i][j] != True:
                    bottom = h - i
                    raise Getoutofloop()
    except Getoutofloop:
        pass

    cropImg = image.crop((0, top, w, bottom))
    cropImg= cropImg.resize((rew,reh))

    return cropImg

class Getoutofloop(Exception):
    pass
</code></pre><p><strong>图片字符标记</strong><br>即给验证码标注正确的字符</p>
<p>到此验证码的预处理工作就大概完成了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要讨论下验证码识别之前一般都会做的验证码的预处理&lt;/p&gt;
&lt;p&gt;验证码的预处理一般有一下几个步骤  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图片去噪（去干扰点及干扰线）  &lt;/li&gt;
&lt;li&gt;将彩色图片二值化为黑白图片  &lt;/li&gt;
&lt;li&gt;图片字符切割    &lt;/li&gt;
&lt;li
    
    </summary>
    
      <category term="python,Machine Learning" scheme="http://zhaoxj0217.github.io/categories/python-Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="captcha" scheme="http://zhaoxj0217.github.io/tags/captcha/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的范数及各种距离—数学基础</title>
    <link href="http://zhaoxj0217.github.io/2017/11/16/norm&amp;Euclidean_distance/"/>
    <id>http://zhaoxj0217.github.io/2017/11/16/norm&amp;Euclidean_distance/</id>
    <published>2017-11-16T06:47:22.245Z</published>
    <updated>2017-11-16T10:10:47.189Z</updated>
    
    <content type="html"><![CDATA[<p>转自<a href="http://blog.csdn.net/shijing_0214/article/details/51757564" target="_blank" rel="external">http://blog.csdn.net/shijing_0214/article/details/51757564</a>  </p>
<p><strong>1.什么是范数</strong><br>&emsp;&emsp;范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。  </p>
<p>&emsp;&emsp;有时候为了便于理解，我们可以把范数当作距离来理解。   </p>
<p>&emsp;&emsp;在数学上，范数包括向量范数和矩阵范数，<strong>向量范数</strong>表征向量空间中向量的大小，<strong>矩阵范数</strong>表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算AX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。</p>
<p><strong>1、 L-P范数</strong><br>&emsp;&emsp;在机器学习中一般使用L-P向量范数，L-P范数不是一个范数，而是一组范数，其定义如下：<br><img src="https://i.loli.net/2017/11/16/5a0d3dcb5161b.png" alt="norm_pic1.png"></p>
<p>根据P 的变化，范数也有着不同的变化，一个经典的有关P范数的变化图如下：<br><img src="https://i.loli.net/2017/11/16/5a0d40fe578e6.png" alt="norm_pic2.png"></p>
<p>上图表示了p从无穷到0变化时，三维空间中到原点的距离（范数）为1的点构成的图形的变化情况。以常见的L-2范数（p=2）为例，此时的范数也即欧氏距离，空间中到原点的欧氏距离为1的点构成了一个球面。  </p>
<p>实际上，在0≤p&lt;1时，Lp并不满足三角不等式的性质，也就不是严格意义下的范数。以p=0.5，二维坐标(1,4)、(4,1)、(1,9)为例，不满足三角不等式。因此这里的L-P范数只是一个概念上的宽泛说法。</p>
<p>闵（明）可夫斯基距离(Minkowski Distance)，闵氏距离（明式）不是一种距离，而是一组距离的定义，对应Lp范数<br>等于1时候，其公式等价于曼哈顿距离。<br>等于2时候，其公式等价于欧式距离。<br>当大于2到无穷大时候，其公式等价于切比雪夫距离。</p>
<p><strong>2、L0范数</strong><br>&emsp;&emsp;当P=0时，也就是L0范数，由上面可知，L0范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数。用上面的L-P定义可以得到的L-0的定义为：<br><img src="https://i.loli.net/2017/11/16/5a0d44d13d6cb.png" alt="norm_pic3.PNG"></p>
<p>这里就有点问题了，我们知道非零元素的零次方为1，但零的零次方，非零数开零次方都是什么鬼，很不好说明L0的意义，所以在通常情况下，大家都用的是： ||x||0=#(i|xi≠0)<br>表示向量x中非零元素的个数。<br>对于L0范数，其优化问题为：<br><img src="https://i.loli.net/2017/11/16/5a0d451e0d809.png" alt="norm_pic4.PNG"></p>
<p>在实际应用中，由于L0范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。所以在实际情况中，L0的最优问题会被放宽到L1或L2下的最优化。</p>
<p><strong>3、L1范数</strong><br>&emsp;&emsp;L1范数是我们经常见到的一种范数，它的定义如下：<br>&emsp;&emsp;||x||1=∑i|xi|<br>&emsp;&emsp;表示向量x中非零元素的绝对值之和。<br>&emsp;&emsp;L1范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）：<br>SAD(x1,x2)=∑i|x1i−x2i|<br>&emsp;&emsp;对于L1范数，它的优化问题如下：<br><img src="https://i.loli.net/2017/11/16/5a0d45d48c9b4.png" alt="norm_pic5.PNG"><br>&emsp;&emsp;由于L1范数的天然性质，对L1优化的解是一个稀疏解，因此L1范数也被叫做稀疏规则算子。通过L1可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有100个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用L1范数就可以过滤掉。</p>
<p><strong>4、L2范数</strong><br>&emsp;&emsp;L2范数是我们最常见最常用的范数了，我们用的最多的度量距离欧氏距离就是一种L2范数，它的定义如下<br><img src="https://i.loli.net/2017/11/16/5a0d482636904.png" alt="norm_pic6.PNG"></p>
<p>&emsp;&emsp;表示向量元素的平方和再开平方。<br>&emsp;&emsp;像L1范数一样，L2也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）: SSD(x1,x2)=∑i(x1i−x2i)2</p>
<p>&emsp;&emsp;对于L2范数，它的优化问题如下：<br><img src="https://i.loli.net/2017/11/16/5a0d4860a15c2.png" alt="norm_pic7.PNG"></p>
<p>&emsp;&emsp;L2范数通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。</p>
<p>欧式距离的向量运算的形式：<br><img src="http://img.my.csdn.net/uploads/201211/20/1353399664_2255.png" alt="norm_pic10.PNG"></p>
<p><strong>5、L-∞范数</strong><br>&emsp;&emsp;当P=∞时，也就是L-∞范数，它主要被用来度量向量元素的最大值。用上面的L-P定义可以得到的L∞的定义为：<br><img src="https://i.loli.net/2017/11/16/5a0d48b452e16.png" alt="norm_pic8.PNG">   </p>
<p>&emsp;&emsp;与L0一样，在通常情况下，大家都用的是：||x||∞=max(|xi|) 来表示L∞<br>&emsp;&emsp;无穷范数对应切比雪夫距离：若二个向量或二个点x1和x2，其坐标分别为(x11, x12, x13, … , x1n)和(x21, x22, x23, … , x2n)，则二者的切比雪夫距离为：d = max(|x1i - x2i|)，i从1到n</p>
<p>&emsp;&emsp;上句摘自维基百科，但是这玩意鬼看得懂啊，为了更好的理解切比雪夫距离，在这里举一个通俗易懂的例子：<br>&emsp;&emsp;比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。<br>&emsp;&emsp;但是按照切比雪夫距离，这是完全不同的概念了。<br>&emsp;&emsp;譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。<br>&emsp;&emsp;这样来看的话，距离是不是就不一样了呢？<br>&emsp;&emsp;或者还是不清楚，再说的详细点。<br>&emsp;&emsp;同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。</p>
<p><strong>其他：汉明距离</strong><br>&emsp;&emsp;汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。</p>
<p><img src="https://i.loli.net/2017/11/16/5a0d496f50d6e.png" alt="norm_pic9.PNG"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转自&lt;a href=&quot;http://blog.csdn.net/shijing_0214/article/details/51757564&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/shijing_0214/a
    
    </summary>
    
      <category term="mathematics,Machine Learning" scheme="http://zhaoxj0217.github.io/categories/mathematics-Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="mathematics" scheme="http://zhaoxj0217.github.io/tags/mathematics/"/>
    
      <category term="norm" scheme="http://zhaoxj0217.github.io/tags/norm/"/>
    
  </entry>
  
  <entry>
    <title>基于svm的验证码识别—svm</title>
    <link href="http://zhaoxj0217.github.io/2017/11/14/captch_by_svm/"/>
    <id>http://zhaoxj0217.github.io/2017/11/14/captch_by_svm/</id>
    <published>2017-11-14T10:34:43.717Z</published>
    <updated>2017-11-29T06:48:27.832Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p>
<p>支持向量机理论部分函数，图片都主要参考<br><a href="http://blog.pluskid.org/?page_id=683" target="_blank" rel="external">pluskid的SVM系列</a><br><a href="http://blog.csdn.net/on2way/article/details/47729419" target="_blank" rel="external">解密SVM系列</a><br><a href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html" target="_blank" rel="external">简易解说拉格朗日对偶</a><br><a href="http://blog.sina.com.cn/s/blog_4298002e010144k8.html" target="_blank" rel="external">支持向量机推导过程</a>  </p>
<p>使用到的概念：<br>L2范数<br>拉格朗日乘子法<br>KKT条件<br>强对偶<br>SMO算法</p>
<p><strong>简单了解SVM</strong><br>&emsp;&emsp;给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为<br><img src="http://img.blog.csdn.net/20131107201211968" alt="">(w为法向量，b为常量)当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，  </p>
<p>(即线性方程:<br><img src="https://i.loli.net/2017/11/17/5a0eacdf359a1.png" width="300" height="30">)   </p>
<p>支持向量到超平面的距离：<br><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%7C%7Cw%7C%7C%7D%28w%5ETx_0%2Bb%29" width="150" height="40"><br>其中||w||为w的二阶范数（2阶范数简单理解即是欧式距离）<br>（这样简单类推这个公式，在二维平面中，点到直线的距离 公式为<img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=930595087,3467127615&amp;fm=58" alt=""> ）</p>
<p>现在已知支持向量到超平面的距离公式，我们要怎么样找到一个最优的超平面。<br>从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。</p>
<p> 如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，这个距离便是几何间隔r，两条虚线间隔边界之间的距离等于2r，而虚线间隔边界上的点则是支持向量。令函数间隔r等于1（之所以令等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响)由于这些支持向量刚好在虚线间隔边界上，所以它们满足<br><img src="http://img.blog.csdn.net/20131111155244218" alt=""><br>而对于所有不是支持向量的点，则显然有<br><img src="http://img.blog.csdn.net/20131111155205109" alt=""></p>
<p><img src="http://img.blog.csdn.net/20140829141714944" alt=""></p>
<p>从而上述目标函数转化成了<br><img src="http://img.my.csdn.net/uploads/201210/25/1351141837_7366.jpg" alt=""></p>
<p>上述目标函数等价于（w由分母变成分子，从而也有原来的max问题变为min问题，很明显，两者问题等价）：<br><img src="http://img.my.csdn.net/uploads/201210/25/1351141994_1802.jpg" alt=""> </p>
<p> 因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的QP (Quadratic Programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。   </p>
<p><strong>深入SVM</strong><br>推导过程比较蛋疼，待研究<br>在求取有约束条件的优化问题时，拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是非常重要的两个求取方法，对于等式约束的优化问题，可以应用拉格朗日乘子法去求取最优值；如果含有不等式约束，可以应用KKT条件去求取。当然，这两个方法求得的结果只是必要条件，只有当是凸函数的情况下，才能保证是充分必要条件。</p>
<p>一. 拉格朗日乘子法（Lagrange Multiplier) 和KKT条件</p>
<p>通常我们需要求解的最优化问题有如下几类：</p>
<p>(i) 无约束优化问题，可以写为:</p>
<pre><code>min f(x);  
</code></pre><p>(ii) 有等式约束的优化问题，可以写为:</p>
<pre><code>min f(x), 
s.t. h_i(x) = 0; i =1, ..., n 
</code></pre><p>(iii) 有不等式约束的优化问题，可以写为：</p>
<pre><code>min f(x), 
s.t. g_i(x) &lt;= 0; i =1, ..., n
h_j(x) = 0; j =1, ..., m
</code></pre><p>对于第(i)类的优化问题，使用求取f(x)的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。</p>
<p>对于第(ii)类的优化问题，常常使用的方法就是拉格朗日乘子法（Lagrange Multiplier) ，即把等式约束h_i(x)用一个系数与f(x)写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。</p>
<p>对于第(iii)类的优化问题，常常使用的方法就是KKT条件。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的必要条件，这个条件称为KKT条件。KKT条件是满足强对偶条件的优化问题的必要条件</p>
<p>那么KKT条件的定理是什么呢？就是如果一个优化问题在转变完后变成<br><img src="https://i.loli.net/2017/11/21/5a13936dd579a.png" alt="svm_pic3.PNG"></p>
<p>其中g是不等式约束，h是等式约束（像上面那个只有不等式约束，也可能有等式约束）。那么KKT条件就是函数的最优值必定满足下面条件：</p>
<p>(1) L对各个x求导为零；<br>(2) h(x)=0;<br>(3) ∑αigi(x)=0，αi≥0<br>关于KKT条件的推导 <a href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html" target="_blank" rel="external">简易解说拉格朗日对偶（Lagrange duality）</a></p>
<p>原问题优化为<br><img src="http://img.my.csdn.net/uploads/201210/25/1351142114_6643.jpg" alt=""><br>对w和b求偏导得到<br><img src="http://img.blog.csdn.net/20131107202220500" alt=""><br>将偏导得到的代入原方程得到<br><img src="http://img.my.csdn.net/uploads/201210/25/1351142449_6864.jpg" alt=""></p>
<p>（2）求对α的极大，即是关于对偶问题的最优化问题。经过上面第一个步骤的求w和b，得到的拉格朗日函数式子已经没有了变量w，b，只有α。从上面的式子得到：<br><img src="http://img.my.csdn.net/uploads/201206/02/1338605996_4659.jpg" alt=""></p>
<p>在求出a以后根据<br><img src="http://img.my.csdn.net/uploads/201301/11/1357838666_9138.jpg" alt="">求出w<br>根据<br><img src="http://img.my.csdn.net/uploads/201301/11/1357838696_3314.png" alt="">求出b</p>
<p>原超平面可表示为<br><img src="https://i.loli.net/2017/11/23/5a168c634b8f0.png" alt="svm_pic4.PNG"></p>
<p><strong>核函数</strong></p>
<p><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/two_circles.png" alt=""><br>前面我们介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要它是线性的，就没法处理，SVM 也不行。因为这样的数据本身就是线性不可分的。<br>如果用 X1 和 X2 来表示这个二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：<br><img src="https://i.loli.net/2017/11/23/5a16906485992.png" alt="svm_pic5.PNG"></p>
<p>注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为 Z1=X1, Z2=X21, Z3=X2, Z4=X22, Z5=X1X2，那么显然，上面的方程在新的坐标系下可以写作：<br><img src="https://i.loli.net/2017/11/23/5a16913d93bb4.png" alt="svm_pic6.PNG"><br>关于新的坐标 Z ，这正是一个超平面的方程！<br>将 X 按照上面的规则映射为 Z ，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。<br>们通过一个映射 ϕ(⋅) 将其映射到一个高维空间中，数据变得线性可分了<br><img src="https://i.loli.net/2017/11/23/5a1691d23687e.png" alt="svm_pic7.PNG"></p>
<p>刚才的方法稍想一下就会发现有问题：在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间（验算一下？），这个数目是呈爆炸性增长的，这给 ϕ(⋅) 的计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。所以就需要 Kernel 出马了</p>
<p>设两个向量 x1=(η1,η2)T 和 x2=(ξ1,ξ2)T ，而 ϕ(⋅) 即是到前面说的五维空间的映射，因此映射过后的内积为：<br><img src="https://i.loli.net/2017/11/23/5a16939e162e9.png" alt="svm_pic8.PNG"><br>同时<br>  <img src="https://i.loli.net/2017/11/23/5a16943560a92.png" alt="svm_pic9.PNG">  </p>
<p>二者有很多相似的地方。区别在于什么地方呢？一个是映射到高维空间中，然后再根据内积的公式进行计算；而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。回忆刚才提到的映射的维度爆炸，在前一种方法已经无法计算的情况下，后一种方法却依旧能从容处理，甚至是无穷维度的情况也没有问题。</p>
<p>我们把这里的计算两个向量在映射过后的空间中的内积的函数叫做核函数 (Kernel Function) ，例如，在刚才的例子中，我们的核函数为：<br><img src="https://i.loli.net/2017/11/23/5a169594b3714.png" alt="svm_pic10.PNG"></p>
<p>核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。对比刚才我们写出来的式子，现在我们的分类函数为：</p>
<p><img src="https://i.loli.net/2017/11/23/5a16987ceecfb.png" alt="svm_pic11.PNG"></p>
<p>其中 α 由如下 dual 问题计算而得：<br><img src="https://i.loli.net/2017/11/23/5a16997fd805c.png" alt="svm_pic12.PNG"><br>这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算，而结果却是等价的</p>
<p>通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数）<br>多项式核：<br>高斯核：<br>线性核：</p>
<p><strong>松弛变量</strong></p>
<p><strong>通过SMO算法求出α</strong></p>
<p><strong>验证码识别</strong><br>验证码的处理跟之前KNN的处理一样，svm算法使用sklearn包里的svm算法</p>
<pre><code># SVM Classifier using cross validation
def svm_cross_validation(train_x, train_y):
    from sklearn.model_selection import GridSearchCV
    from sklearn.svm import SVC

    model = SVC(kernel=&apos;rbf&apos;, probability=True,decision_function_shape=&apos;ovo&apos;)
    param_grid = {&apos;C&apos;: [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], &apos;gamma&apos;: [0.001, 0.0001]}
    grid_search = GridSearchCV(model, param_grid, n_jobs=1, verbose=1)
    grid_search.fit(train_x, train_y)
    best_parameters = grid_search.best_estimator_.get_params() #获取最佳参数
    for para, val in best_parameters.items():
        print
        para, val
    model = SVC(kernel=&apos;rbf&apos;, C=best_parameters[&apos;C&apos;], gamma=best_parameters[&apos;gamma&apos;], probability=True,decision_function_shape=&apos;ovo&apos;)
    model.fit(train_x, train_y)
        return model
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。&lt;/p&gt;
&lt;p&gt;
    
    </summary>
    
      <category term="python,SVM,Machine Learning" scheme="http://zhaoxj0217.github.io/categories/python-SVM-Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="SVM" scheme="http://zhaoxj0217.github.io/tags/SVM/"/>
    
      <category term="captcha" scheme="http://zhaoxj0217.github.io/tags/captcha/"/>
    
  </entry>
  
  <entry>
    <title>基于K近邻算法的验证码识别—knn</title>
    <link href="http://zhaoxj0217.github.io/2017/11/07/captch_by_kNN/"/>
    <id>http://zhaoxj0217.github.io/2017/11/07/captch_by_kNN/</id>
    <published>2017-11-07T06:44:42.194Z</published>
    <updated>2017-11-16T10:33:38.489Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于K近邻算法进行验证码识别，是验证码识别中最简单的一种机器学习方法</p>
<p><strong>1.1、什么是K近邻算法</strong></p>
<p>&emsp;&emsp;所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。<br>咱们来看下引自维基百科上的一幅图：<br><img src="http://img.my.csdn.net/uploads/201211/20/1353395335_6987.png" alt="knn_pic1.png">   </p>
<p> &emsp;&emsp;如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。我们就要解决这个问题：给这个绿色的圆分类。<br> &emsp;&emsp;如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。<br> &emsp;&emsp;如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。<br>  &emsp;&emsp;于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</p>
<p><strong>近邻的距离度量表示法</strong></p>
<p>&emsp;&emsp;K近邻算法的核心在于找到实例点的邻居，这个时候，问题就是如何找到邻居，邻居的判定标准是什么，用什么来度量。特征空间中两个实例点的距离可以反应出两个实例点之间的相似性程度，K近邻模型的特征空间一般是n维实数向量空间，使用的距离可以使最常用的欧式距离，也是可以是其它距离（汉明距离，闵可夫斯基距离，切比雪夫距离，曼哈顿距离，这里又可以扩展到范数的概念）</p>
<p><strong>K值的选择</strong></p>
<p>除了上述如何定义邻居的问题之外，还有一个选择多少个邻居，即K值定义为多大的问题。不要小看了这个K值选择问题，因为它对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：<br>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；<br>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。<br>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。<br>在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p>
<p><strong>基本流程</strong><br>一般情况下，对于字符型验证码的识别流程如下：</p>
<ul>
<li>准备原始验证码素材  </li>
<li>图片预处理</li>
<li>生成训练数据集  </li>
<li>训练模型（KNN不涉及） </li>
<li>测试验证码识别率 </li>
</ul>
<p> <strong>素材选择</strong><br>选择需要识别的验证码，验证码的复杂程度不影响上述的基本流程，所以选择一个比较简单的验证码进行示范</p>
<p><img src="https://i.loli.net/2017/11/16/5a0d51e2715c3.jpg" alt="9639.jpg"></p>
<p>该验证码由4位阿拉伯数组组成，没有干扰线，没有背景噪点<br>因机器学习过程需要大量的样本，样本的获取可以去目标网站获取，该方式的缺点是需要手动标记，另一种是自己用代码批量生成，前提是你已了解了对方验证码生成的方式（或者近似的方式）</p>
<p><strong>图片预处理</strong></p>
<p>为了减少后面训练时的复杂度，同时增加识别率，很有必要对图片进行预处理，使其对机器识别更友好。</p>
<p>图片预处理的过程请参考站内另一文章： 验证码图片预处理—python</p>
<p><strong>生成训练数据集</strong>  </p>
<p>将分割完毕的字符并标记好的样本转化为数学向量导入内存中作为样本数据</p>
<pre><code>def createDataSet():
    print(&quot;start init train data...&quot;)
    global group
    global lables

    for number in range(10):
        im_paths = filter(lambda fn: os.path.splitext(fn)[1].lower() == &apos;.png&apos;,os.listdir(&quot;./knnImage/letters/&quot;+str(number)))

    for im_path in im_paths:
        imagepath = &quot;./knnImage/letters/&quot; +str(number)+&quot;/&quot;+ im_path
        image = Image.open(imagepath)
        try:
            group.append(img2vector(image))
            lables.append(number)
        except:
            print(im_path,&quot;createDataSetError&quot;)
        pass
#转为numpy数组
    group = np.array(group)
    lables = np.array(lables)
    print(&quot;end init train data...&quot;)

#使用到的图片转向量函数
#图片转向量
def img2vector(image):
    returnVect =[]
    img_raw = np.array(image)
    width, height = image.size
    for h in range(height):
        for w in range(width):
            if img_raw[h][w] ==True:#因为二值化图片的结果是True跟False,这里转化为01
                returnVect.append(0)
            else:
                returnVect.append(1)
    return returnVect
</code></pre><p><strong>训练模型</strong><br>knn算法不涉及模型训练</p>
<p><strong>验证码识别</strong></p>
<p> 识别验证码  </p>
<pre><code>def getCaptcha(image):
    # img_raw = np.array(image)
    global group
    global lables
    w, h = image.size
    splitCol = getSplit(image)

    if len(splitCol) != 4:
        print(&quot;切割错误！&quot;, text)
        return &quot;&quot;

    chars = []
    for i in range(4):
        cropImg = image.crop((splitCol[i][0], 0, splitCol[i][1], h))
        cropImg = resizeImge(cropImg, int(w /4), h)  # 切割的图片尺寸不一，统一尺寸
        charvec = img2vector(cropImg)
        char =  classify(charvec, group, lables, 3)
        chars.append(str(char))
    return &quot;&quot;.join(chars)
</code></pre><p><strong>knn算法</strong></p>
<pre><code># knn分类算法 此处使用欧式距离公式
#inX :待分类向量
#dataSet：训练集合
#labels：训练集合对应的分类结果
#k值
def classify(inX, dataSet, labels, k):
    # 距离计算 start,
    dataSetSize = dataSet.shape[0]# 计算有多少个训练样本
    tmp = np.tile(inX, (dataSetSize, 1))# 将待分类的输入向量进行 行反向复制dataSetSize次，列方向复制1一次,即与训练样本大小一致
    diffMat = tmp - dataSet # 数组相减
    sqDiffMat = diffMat ** 2 #2次方
    sqDistances = sqDiffMat.sum(axis=1)# 对于二维数组axis=1表示按行相加 , axis=0表示按列相加
    distances = sqDistances ** 0.5 # 平方根
    # 距离计算 end
    sortedDistIndicies = distances.argsort() # 将输入与训练集的距离排序 argsort函数返回的是数组值从小到大的索引值
    classCount = {}
    for i in range(k): # 统计距离最近的K个lable值
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
    sortedClassCount = sorted(classCount.items(), key=lambda item: item[1], reverse=True)
    return sortedClassCount[0][0]#返回k个中数量最多的label
</code></pre><p>测试knn分类的效果</p>
<pre><code>if __name__ == &quot;__main__&quot;:

# 图片识别  --start
#创建训练集合
    createDataSet()#训练数据加载比较慢

    im_paths = filter(lambda fn: os.path.splitext(fn)[1].lower() == &apos;.jpg&apos;,os.listdir(&quot;./knnImage/test&quot;))

    total = 0
    right = 0
    for im_path in im_paths:
        total+=1
         imagepath = &quot;./knnImage/test/&quot; + im_path
        image = Image.open(imagepath)
        imgry = image.convert(&apos;L&apos;)  # 转化为灰度图
        table = get_bin_table(230)
        out = imgry.point(table, &apos;1&apos;)
        text = os.path.basename(im_path).split(&apos;_&apos;)[0]
        knntext = getCaptcha(out)
        if text == knntext:
               right+=1
            print(&quot;knn right&quot;)
        else:
            print(text,&quot;knn error:&quot;,knntext)
    print(&quot;right:total is:&quot;,right,total)    
# 图片识别  --end
</code></pre><p>如果测试结果能满足预期的识别率，就可以使用该训练样本去实际使用了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于K近邻算法进行验证码识别，是验证码识别中最简单的一种机器学习方法&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.1、什么是K近邻算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最
    
    </summary>
    
      <category term="python,KNN,Machine Learning" scheme="http://zhaoxj0217.github.io/categories/python-KNN-Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="captcha" scheme="http://zhaoxj0217.github.io/tags/captcha/"/>
    
      <category term="KNN" scheme="http://zhaoxj0217.github.io/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>深入理解spring事物机制—transactional</title>
    <link href="http://zhaoxj0217.github.io/2017/11/06/spring_Transactional/"/>
    <id>http://zhaoxj0217.github.io/2017/11/06/spring_Transactional/</id>
    <published>2017-11-06T09:27:02.432Z</published>
    <updated>2017-11-07T02:38:59.061Z</updated>
    
    <content type="html"><![CDATA[<h2 id="事务是什么？"><a href="#事务是什么？" class="headerlink" title="事务是什么？"></a>事务是什么？</h2><p><strong>&emsp;- 狭义上的理解：关系型数据库事务<br>&emsp;- ACID：原子性、一致性、隔离性、持久性<br>&emsp;- 为什么我们要用“事务”？<br>&emsp;- 测试的盲点</strong>  </p>
<p><strong>首先问几个问题</strong></p>
<p><em>问题一：</em></p>
<pre><code>@Transactional(propagation = Propagation.REQUIRED)
A.test();

@Transactional(propagation = Propagation.REQUIRED)
B.test();

@Transactional(propagation = Propagation.REQUIRED)
Entry.test(){
    try{
        A.test();
    }catch(Exception e){
        //这里捕获异常会回滚Entry.test事物么？
    }
    B.test();
}
</code></pre><p><em>问题二：</em></p>
<pre><code>@Transactional(propagation = Propagation.REQUIRES_NEW)
A.test();

@Transactional(propagation = Propagation.REQUIRED)
B.test();

@Transactional(propagation = Propagation.REQUIRED)
Entry.test(){
    try{
        A.test();
    }catch(Exception e){
        //这里捕获异常会回滚Entry.test事物么？
    }
    B.test();
}
</code></pre><p><em>问题三：</em></p>
<pre><code>@Transactional(propagation = Propagation.REQUIRES_NEW)
A.test();

@Transactional(propagation = Propagation.REQUIRED)
B.test();

@Transactional(propagation = Propagation.REQUIRED)
Entry.test(){
     A.test();
    B.test();
    try{
       throw new RuntimeException()
    }catch(Exception e){
        //这里捕获异常会回滚Entry.test事物么？
    }

}
</code></pre><p><em>问题四：</em></p>
<pre><code>public class A {

@Transactional(propagation = Propagation.REQUIRED)
public void testA1(){
    try{
        this.testA2();
    }catch (Exception e){
        //这里捕获异常会回滚testA1的事务么
    }
}

@Transactional(propagation = Propagation.REQUIRED)
public void testA2(){
}
}
</code></pre><p><em>问题五：</em></p>
<pre><code>@Transactional(timeout = 5)
Entry.test(){
    Thread.sleep(6000);
    A.test();//执行数据库操作，耗时1秒
}
//超时会导致Entry.test事务回滚么？
</code></pre><p><em>问题六：</em></p>
<pre><code>@Transactional(timeout = 5)
Entry.test(){       
    A.test();//执行数据库操作，耗时1秒
     Thread.sleep(6000);
}
//超时会导致Entry.test事务回滚么？
</code></pre><p><em>问题七：</em></p>
<pre><code>@Transactional()
Entry.test(){       
    A.test();//执行数据库操作，在并发情况下会争夺行锁
}
//为什么没有配置超时，却还是捕捉到了超时异常：
###Cause:java.sql.SQLException:Lock wait timeout exceeded; try restarting transaction
</code></pre><p><em>问题八：</em><br>    &emsp;@Transactional()应该加在实现类的方法上还是接口的方法上?</p>
<p><em>问题九：</em><br>    &emsp;@Transactional()加在private、final、protected方法上有用吗?</p>
<h2 id="事务的代码流转"><a href="#事务的代码流转" class="headerlink" title="事务的代码流转"></a>事务的代码流转</h2><p>&emsp;&emsp;spring事务管理器-&gt;mybatis-&gt;jdbc-&gt;mysql</p>
<p>MySQL（InnoDB）对事务的支持:<br>&emsp;begin:<br>&emsp;savepoint:<br>&emsp;rollback:<br>&emsp;commit:  </p>
<p>MySQL（InnoDB）不支持事务嵌套，第二个begin会隐式地把第一个给先commit掉</p>
<p>Jdbc对应的api</p>
<pre><code>java.sql.Driver.java:
    -Connection connect(String url, java.util.Properties info)throws SQLException; //获取数据库连接

java.sql.Connection.java:
    -void setAutoCommit(boolean autoCommit) throws SQLException;//设置为false表示事务begin
    -Savepoint setSavepoint() throws SQLException; //创建保存点Savepoint 
    -setSavepoint(String name) throws SQLException; //创建保存点void 
    -rollback(Savepoint savepoint) throws SQLException; //回滚保存点void 
    -rollback() throws SQLException; //回滚整个事务void 
    -releaseSavepoint(Savepoint savepoint) throws SQLException;//删除保存点
    -void commit() throws SQLException; //提交整个事务Statement 
    -createStatement() throws SQLException;//创建语句

java.sql.Statement.java:
    -int executeUpdate(String sql) throws SQLException;//执行sql更新语句，当然还有其他执行接口
</code></pre><p>Savepoint的操作要jdbc3.0及以上才支持</p>
<p><strong>mysql 事务的三大属性</strong>  </p>
<ul>
<li>隔离级别  <table><pre><code>&lt;tr&gt;
    &lt;th&gt;隔离级别&lt;/th&gt;
    &lt;th&gt;脏度可能性&lt;/th&gt;
    &lt;th&gt;不可读可能性&lt;/th&gt;
    &lt;th&gt;幻读可能性&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;th&gt;READ UNCOMMITTED&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;th&gt;READ COMMITTED&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;th&gt;REPEATABLE READ&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
    &lt;th&gt;是&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;th&gt;SERIALIZABLE&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
    &lt;th&gt;否&lt;/th&gt;
&lt;/tr&gt;
</code></pre> </table></li>
</ul>
<p>Mysql默认隔离级别是REPEATABLE READ<br>查看命令：select @@tx_isolation</p>
<blockquote>
<p>脏读：读取的是别的事务的半成品，不会用于生产环境。  </p>
<p>不可重复读：是指在一个事务内，多次读同一行数据。在这个事务还没有结束时，另外一个事务修改该行数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。</p>
<p>可重复读：就是不可重复读的取反</p>
<p>幻读:是指在一个事务内，按条件读取几行数据。在这个事务还没有结束时，另外一个事务插入或者删除了符合条件的数据。那么，在第一个事务中的两次相同条件的读取数据之间，由于第二个事务的插入和删除，那么第一个事务两次读到的的数据行数可能是不一样的。</p>
<blockquote>
<p>幻读对代码编写方式的影响<br> CRIRATE TABLE ‘tb_trans_student’(<br>   ‘Id’ bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT ‘Id’，<br>   ‘Name’ varchar(255) NOT NULL COMMENT ‘姓名’，<br>   ‘Score’ smallint(3) unsigned NOT NULL COMMENT ‘分数’，<br>   PRIMARY KEY (‘Id’)，<br>   UNIQUE KEY ‘Name’ (‘Name’)<br> )ENGINE = InnoBD AUTO_INCREMENT =18 DEFAULT CHARSET =utf8 COMMENT =’学生表’  </p>
</blockquote>
</blockquote>
<pre><code>@Transactional
public void insert(Object data){
    try{
        Object dataInDB = db.selectWhere(data.name);    
        if(dataInDB== null){
            db.insert(data);//另一个线程刚刚插入，导致这里会违反唯一性约束
        }
    }catch(Exception e){
        Object dataInDB = db.selectWhere(data.name);
        log(&quot;数据库已存在&quot;+dataInDB);//这里dataInDB为null
    }    
}
</code></pre><blockquote>
<blockquote>
<p>插又插不进去，读也读不到</p>
</blockquote>
</blockquote>
<p><strong>Jdbc对应的api(事务隔离级别)</strong>  </p>
<ul>
<li>java.sql.Connection.java:  </li>
<li>int TRANSACTION_READ_UNCOMMITTED = 1;  </li>
<li>int TRANSACTION_READ_COMMITTED   = 2 ;  </li>
<li>int TRANSACTION_REPEATABLE_READ  = 4;  </li>
<li>int TRANSACTION_SERIALIZABLE     = 8;  </li>
<li>void setTransactionIsolation(int level) throws SQLException;//设置隔离级别，需要在begin之前执行  </li>
<li>int getTransactionIsolation() throws SQLException; </li>
</ul>
<ul>
<li><p>超时<br><strong>Jdbc对应的api(事务超时)</strong><br>– 只有Statement有一个设置sql执行超时时间的api<br>– java.sql.Statement.java:<br>   void setQueryTimeout(int seconds) throws SQLException;//名字取得不好，会让人误以为只是设置查询超时，实际update也起作用</p>
</li>
<li><p>只读<br><strong>Jdbc对应的api(事务只读)</strong><br>– java.sql.Connection.java:<br>– void setReadOnly(boolean readOnly) throws SQLException;//设置是否只读，需要在begin之前执行<br>– boolean isReadOnly() throws SQLException;  </p>
</li>
</ul>
<h2 id="Spring事务框架"><a href="#Spring事务框架" class="headerlink" title="Spring事务框架"></a>Spring事务框架</h2><p><img src="http://img.blog.csdn.net/20160324011156424" alt=""></p>
<p>&emsp;- 常用的DataSourceTransactionManager </p>
<pre><code>javax.sql.DataSource.java   -&gt; mysql //对应各种数据库  
Connection getConnection(String username, String password)throws SQLException;  

org.springframework.transaction.PlatformTransactionManager.java -&gt; 事务管理器  
TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException;//开启一个事务
void commit(TransactionStatus status) throws TransactionException; //提交一个事务
void rollback(TransactionStatus status) throws TransactionException;//回滚一个事务

org.springframework.transaction.SavepointManager.java -&gt; 保存点管理器Object 
createSavepoint() throws TransactionException; //创建保存点
void rollbackToSavepoint(Object savepoint) throws TransactionException;//回滚保存点void 
releaseSavepoint(Object savepoint) throws TransactionException;//删除保存点
TransactionStatus extends SavepointManager

拿到了TransactionStatus后怎么拿到对应Connection执行sql？
DataSourceTransactionObject txObject = (DataSourceTransactionObject) transactionStatus.getTransaction();
Connection con = txObject.getConnectionHolder().getConnection();
</code></pre><p>&emsp;- Spring事务管理器配置   </p>
<pre><code>&lt;!-- 事务管理器 xml配置--&gt;
&lt;bean id =&apos;transactionManager&apos;
    class=&apos;org.springframework.jdbc.datasource.DataSourceTransactionManager&apos;&gt;
    &lt;property name =&apos;dataSource&apos; ref =&apos;dataSource&apos; /&gt;
    &lt;property name =&apos;validateExistingTransaction&apos; value =&apos;true&apos; /&gt;
&lt;/bean&gt;

&lt;tx:annotation-driven transaction-manager =&apos;transactionManager&apos; proxy-target-class=&apos;true&apos; /&gt;

@Transactional(readOnly =false, rollbackFor = {Exception.class},noRollbackFor ={Error.class},isolation = Isolation.DEFAULT,propagatio =Propagation.REQUIRED,timeout =5)
</code></pre><p>&emsp;- spring事务的五大配置参数<br>&emsp;&emsp;- 是否只读<br>&emsp;&emsp;- 事务的回滚规则(抛出哪些异常时需要回滚，哪些不需要)<br>&emsp;&emsp;- 事务的隔离性<br>&emsp;&emsp;- 事务的传播行为<br>&emsp;&emsp;- 事务的超时  </p>
<p>事务的回滚规则的实现:   </p>
<p>具体实现位于org.springframework.transaction.interceptor. RuleBasedTransactionAttribute.java类中的public boolean rollbackOn(Throwable ex);  </p>
<ul>
<li>先遍历rollbackFor，找到一个最接近的配置  </li>
<li>再遍历noRollbackFor ，找到一个最接近的配置  </li>
<li>如果上面两步找到了一个最接近的配置，属于rollbackFor则回滚，属于noRollbackFor 则不回滚  </li>
<li>如果上面两步都没找到一个最接近的配置，则：</li>
<li>如果是RuntimeException或者Error的子类就回滚，否则不回滚</li>
</ul>
<p>事务的传播行为<table><br>        <tr><br>            <th>类型</th><br>            <th>说明</th><br>        </tr><br>        <tr><br>            <th>Propagation.REQUIRED</th><br>            <th>代表当前方法支持当前的事务，且与调用者处于同一事务上下文中，回滚统一回滚（如果当前方法是被其他方法调用的时候，且调用者本身即有事务），如果没有事务，则自己新建事务，</th><br>        </tr><br>        <tr><br>            <th>Propagation.SUPPORTS</th><br>            <th>代表当前方法支持当前的事务，且与调用者处于同一事务上下文中，回滚统一回滚（如果当前方法是被其他方法调用的时候，且调用者本身即有事务），如果没有事务，则该方法在非事务的上下文中执行</th><br>        </tr><br>        <tr><br>            <th>Propagation.MANDATORY</th><br>            <th>代表当前方法支持当前的事务，且与调用者处于同一事务上下文中，回滚统一回滚（如果当前方法是被其他方法调用的时候，且调用者本身即有事务）,如果没有事务，则抛出异常</th><br>        </tr><br>        <tr><br>            <th>Propagation.REQUIRES_NEW</th><br>            <th>创建一个新的事务上下文，如果当前方法的调用者已经有了事务，则挂起调用者的事务，这两个事务不处于同一上下文，如果各自发生异常，各自回滚</th><br>        </tr><br>        <tr><br>            <th>Propagation.NOT_SUPPORTED</th><br>            <th>该方法以非事务的状态执行，如果调用该方法的调用者有事务则先挂起调用者的事务</th><br>        </tr><br>        <tr><br>            <th>Propagation.NEVER</th><br>            <th>该方法以非事务的状态执行，如果调用者存在事务，则抛出异常</th><br>        </tr><br>        <tr><br>            <th>Propagation.NESTED</th><br>            <th>如果当前上下文中存在事务，则以嵌套事务执行该方法，也就说，这部分方法是外部方法的一部分，调用者回滚，则该方法回滚，但如果该方法自己发生异常，则自己回滚，不会影响外部事务，如果不存在事务，则与PROPAGATION_REQUIRED一样，（其实这是数据库带有保存点的事务的典型体现，举例来说：旅游行业来说，游客从上海飞巴厘岛，需要从香港进行转机，那么上海~香港就是一个方法且是一个事务，香港~巴厘岛就是嵌入的方法，如果香港~巴厘岛航班取消了，无需回滚上海~香港的，这样代价太大，因为游客已经到了香港，只需修改香港到巴厘岛的航班就可以了，如果上海~香港的飞机取消了，则需要全部事务回滚，其实香港到巴厘岛的航班没有问题</th><br>        </tr><br></table></p>
<p>传播行为实现的大致逻辑<br>传播行为的实现逻辑都差不多，第一步都是判断当前是否存在事务。那么，如何判断当前是否存在事务？<br>通过传递函数参数？<br>维护一个全局变量Map，key为当前线程，value不等于null表示已有事务，事务提交或回滚时清除value。如何防止其他线程clear map？ ==》 ThreadLocal   </p>
<p>事务的传播行为</p>
<p><img src="/2017/11/06/spring_Transactional/images/spring_transaction.png" alt=""><br>上图中如果虚线对应的事务发生异常，则不回滚Connection所在的事务；如果实线对应的事务发生异常，则回滚Connection所在的事务。</p>
<p><strong>PROPAGATION_REQUIRES_NEW和PROPAGATION_NESTED</strong></p>
<p>区别和各自使用场景  </p>
<ul>
<li>本质区别：PROPAGATION_REQUIRES_NEW是新开事务，PROPAGATION_NESTED是在当前事务上创建保存点。  </li>
<li>回滚区别：PROPAGATION_REQUIRES_NEW回滚自身事务，不会回滚上层事务；PROPAGATION_NESTED只是回滚到保存点，不会回滚上层事务。  </li>
<li>提交区别：PROPAGATION_REQUIRES_NEW提交自身事务，即使后来上层事务回滚，也不会再回滚自身事务；PROPAGATION_NESTED提交只是删除保存点，只能跟随上层事务一起提交，如果后来上层回滚，也跟着回滚。  </li>
</ul>
<p>PROPAGATION_REQUIRES_NEW是自立门户，上层事务可以容忍自身失败的情况下NEW事务提交成功，但是不容忍NEW事务失败的情况下继续执行上层事务。<br>类似的场景：<br>单独的表用来生成递增的Id，即使Id不连续也没关系，但是上层事务依赖于这个id。  </p>
<p>PROPAGATION_NESTED是没有自立能力的，依附于上层事务，上层事务对它也漠不关心，成功最好，不成功也算了，后续可以根据其他信息推算出来。<br>类似的场景：<br>比如买东西的时候有小票和发票，打印发票属于内嵌事务，打不了发票你照样可以付钱买东西，小票也可以保修。  </p>
<p><strong>事务的超时</strong><br>如何检测一个函数是否超时:<br>定时触发：<br>    框架开个定时器(另开线程)，时间到了触发回调函数，回调函数检查目标函数是否执行完毕。<br>优点是框架时间控制比较精确。<br>缺点是如果超时了，该如何提前终止目标函数？</p>
<p>轮询:<br>目标函数每次在进入阻塞操作或者耗时操作前轮询一下框架自己是否已经超时。<br>优点是开销小、目标函数可以决定在合适的时间和节点提前结束自己的运行。<br>缺点是框架无法精确控制超时，全靠目标函数自觉。  </p>
<p><em>spring采取的是 轮询检测。<br>因此如果@Transactional(timeout = 5) 目标函数在执行时不去轮询自己是否超时，即使过了5秒，还是不会超时的。</em></p>
<p><strong>@Transactional冲突</strong><br>是在事务doBegin的时候设置超时的deadline以及隔离级别、设置是否只读（确切的说隔离级别和是否只读是在创建connection后、begin之前的时候设置的，因为这2个底层是由connection设置的）</p>
<p><strong>@Transactional冲突</strong> </p>
<p>结论：为了一劳永逸，请用在实现类上。<br>annotation加在类上，则jdk代理和cglib代理都可以扫描到。<br>annotation加在接口上，只能在jdk代理的时候扫描到。<br>为什么加在接口上cglib检测不到<br>JDK的动态代理是实现接口方式生成代理类。<br>CGLIB动态代理是通过继承方式生成代理类,底层使用ASM框架生成字节码完成代理功能。<br>Spring AOP默认首先使用JDK动态代理来代理目标对象，如果目标对象没有实现任何接口将使用CGLIB代理，如果需要强制使用CGLIB代理，可以指定</p>
<aop:aspectj-autoproxy proxy-target-class="true">


<p><strong>@Transactional的位置</strong><br>可以放在private、final、 protected方法之上吗？<br>jdk是代理接口，非public方法必然不会存在在接口里，所以就不会被拦截到，也无法在接口里定义final方法；<br>cglib是子类，private的方法照样不会出现在子类里，也不能被拦截。 而且final方法也不能覆盖。   </p>
<p>private、protected如果用到了，则是代码有问题。<br>final在jdk代理下有用，cglib下没用。   </p>
<h2 id="问题答案"><a href="#问题答案" class="headerlink" title="问题答案"></a>问题答案</h2><p>问题1：会。<br>问题2：不会。<br>问题3：不会。<br>问题4：不会。<br>问题5：会。<br>问题6：不会。<br>问题7：超过了数据库innodb_lock_wait_timeout行锁时间。<br>问题8：为了cglib代理能识别，都加在实现类的方法上。<br>问题9：没用且不合理。   </p>
</aop:aspectj-autoproxy>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;事务是什么？&quot;&gt;&lt;a href=&quot;#事务是什么？&quot; class=&quot;headerlink&quot; title=&quot;事务是什么？&quot;&gt;&lt;/a&gt;事务是什么？&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;&amp;emsp;- 狭义上的理解：关系型数据库事务&lt;br&gt;&amp;emsp;- ACID：原子性、一致
    
    </summary>
    
      <category term="spring,transactional,mysql" scheme="http://zhaoxj0217.github.io/categories/spring-transactional-mysql/"/>
    
    
      <category term="spring" scheme="http://zhaoxj0217.github.io/tags/spring/"/>
    
  </entry>
  
  <entry>
    <title>Mysql 执行优化—mysql</title>
    <link href="http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_execute/"/>
    <id>http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_execute/</id>
    <published>2017-11-05T07:39:48.278Z</published>
    <updated>2017-11-06T02:13:36.676Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mysql-执行优化"><a href="#Mysql-执行优化" class="headerlink" title="Mysql 执行优化"></a>Mysql 执行优化</h1><h2 id="认识数据索引"><a href="#认识数据索引" class="headerlink" title="认识数据索引"></a>认识数据索引</h2><p>1). 为什么使用数据索引能提高效率<br>&emsp;•    数据索引的存储是有序的<br>&emsp;•    在有序的情况下，通过索引查询一个数据是无需遍历索引记录的<br>&emsp;•    极端情况下，数据索引的查询效率为二分法查询效率，趋近于 log2(N)<br>2). 如何理解数据索引的结构<br>&emsp;•    数据索引通常默认采用 btree 索引，（内存表也使用了 hash 索引）。<br>&emsp;•    单一有序排序序列是查找效率最高的（二分查找，或者说折半查找），使用树形索引的目的是为了达到快速的更新和增删操作。<br>&emsp;•    在极端情况下（比如数据查询需求量非常大，而数据更新需求极少，实时性要求不高，数据规模有限），直接使用单一排序序列，折半查找速度最快。<br>&emsp;&emsp;&emsp;    实战范例 ： ip 地址反查<br>&emsp;&emsp;&emsp;资源： Ip 地址对应表，源数据格式为  startip, endip, area 源数据条数为 10 万条左右，呈很大的分散性目标： 需要通过任意 ip 查询该 ip 所属地区<br>性能要求达到每秒 1000 次以上的查询效率<br>&emsp;&emsp;&emsp;挑战： 如使用 between … and 数据库操作，无法有效使用索引。<br>如果每次查询请求需要遍历 10 万条记录，根本不行。<br>&emsp;&emsp;&emsp;方法： 一次性排序（只在数据准备中进行，数据可存储在内存序列）折半查找（每次请求以折半查找方式进行）<br>&emsp;•        在进行索引分析和 SQL 优化时，可以将数据索引字段想象为单一有序序列，并以此作为分析的基础。<br>&emsp;&emsp;&emsp;    实战范例：复合索引查询优化实战，同城异性列表<br>资源： 用户表 user，字段 sex 性别；area 地区；lastlogin 最后登录时间；其他略目标： 查找同一地区的异性，按照最后登录时间逆序    高访问量社区的高频查询，如何优化。<br>&emsp;&emsp;&emsp;&emsp;查询 SQL: select * from user where area=’$area’ and sex=’$sex’ order by lastlogin desc limit 0,30;<br>&emsp;&emsp;&emsp;&emsp;挑战： 建立复合索引并不难， area+sex+lastlogin 三个字段的复合索引,如何理解？<br>&emsp;&emsp;&emsp;&emsp;首先，忘掉 btree，将索引字段理解为一个排序序列。<br>&emsp;&emsp;&emsp;&emsp;如果只使用 area 会怎样？搜索会把符合 area 的结果全部找出来，然后在这里面遍历，选择命中 sex 的并排序。 遍历所有 area=’$area’数据！<br>如果使用了 area+sex，略好，仍然要遍历所有 area=’$area’     and sex=’$sex’数据，然后在这个基础上排序！！<br>&emsp;&emsp;&emsp;&emsp;Area+sex+lastlogin 复合索引时（切记 lastlogin 在最后），该索引基于 area+sex+lastlogin 三个字段合并的结果排序，该列表可以想象如下。<br>&emsp;&emsp;&emsp;&emsp;广州女$时间 1 广州女$时间 2 广州女$时间 3<br>&emsp;&emsp;&emsp;&emsp;…<br>&emsp;&emsp;&emsp;&emsp;广州男<br>&emsp;&emsp;&emsp;&emsp;….<br>&emsp;&emsp;&emsp;&emsp;深圳女<br>&emsp;&emsp;&emsp;&emsp;….<br>&emsp;&emsp;&emsp;&emsp;数据库很容易命中到     area+sex 的边界，并且基于下边界向上追溯 30 条记录，搞定！在索引中迅速命中所有结果，无需二次遍历！ </p>
<p>3). 如何理解影响结果集<br>&emsp;•    影响结果集是数据查询优化的一个重要中间数据<br>&emsp;&emsp;&emsp;    查询条件与索引的关系决定影响结果集如上例所示，即便查询用到了索引，但是如果查询和排序目标不能直接在索引中命中，其可能带来较多的影响结果。而这会直接影响到查询效率<br>&emsp;&emsp;&emsp;    微秒级优化<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    优化查询不能只看慢查询日志，常规来说，0.01 秒以上的查询，都是不够优化的。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    实战范例<br>&emsp;&emsp;&emsp;&emsp;&emsp;和上案例类似，某游戏社区要显示用户动态，select * from userfeed where uid=$uid order by lastlogin desc limit 0,30;   初期默认以 uid 为索引字段，查询为命中所有 uid=$uid 的结果按照 lastlogin 排序。 当用户行为非常频繁时，该 SQL 索引命中影响结果集有数百乃至数千条记录。查询效率超过 0.01 秒，并发较大时数据库压力较大。<br>&emsp;&emsp;&emsp;&emsp;&emsp;解决方案：将索引改为 uid+lastlogin 复合索引，索引直接命中影响结果集 30 条，查询效率提高了 10 倍，平均在 0.001 秒，数据库压力骤降。<br>&emsp;&emsp;&emsp;    影响结果集的常见误区<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    影响结果集并不是说数据查询出来的结果数或操作影响的结果数，而是查询条件的索引所命中的结果数。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    实战范例<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        某游戏数据库使用了 innodb，innodb 是行级锁，理论上很少存在锁表情况。出现了一个 SQL 语句(delete from tabname where xid=…)，这个 SQL 非常用 SQL，仅在特定情况下出现，每天出现频繁度不高（一天仅 10 次左右），数据表容量百万级，但是这个 xid 未建立索引，于是悲惨的事情发生了，当执行这条 delete 的时候，真正删除的记录非常少，也许一到两条，也许一条都没有；但是！由于这个 xid 未建立索引，<br>delete 操作时遍历全表记录，全表被 delete 操作锁定，select 操作全部被 locked，由于百万条记录遍历时间较长，期间大量 select 被阻塞，数据库连接过多崩溃。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这种非高发请求，操作目标很少的 SQL，因未使用索引，连带导致整个数据库的查询阻塞，需要极大提高警觉。<br>&emsp;&emsp;&emsp;    总结：<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    影响结果集是搜索条件索引命中的结果集，而非输出和操作的结果集。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    影响结果集越趋近于实际输出或操作的目标结果集，索引效率越高。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    请注意，我这里永远不会讲关于外键和 join 的优化，因为在我们的体系里，这是根本不允许的！ 架构优化部分会解释为什么。  </p>
<h2 id="理解执行状态"><a href="#理解执行状态" class="headerlink" title="理解执行状态"></a>理解执行状态</h2><p><strong>常见分析手段</strong> </p>
<p>&emsp;•    慢查询日志，关注重点如下<br>&emsp;&emsp;&emsp;    是否锁定，及锁定时间<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如存在锁定，则该慢查询通常是因锁定因素导致，本身无需优化，需解决锁定问题。<br>&emsp;&emsp;&emsp;    影响结果集<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如影响结果集较大，显然是索引项命中存在问题，需要认真对待。  </p>
<p>&emsp;•    Explain 操作<br>&emsp;&emsp;&emsp;    索引项使用<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    不建议用 using index 做强制索引，如未如预期使用索引，建议重新斟酌表结构和索引设置。<br>&emsp;&emsp;&emsp;    影响结果集<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    这里显示的数字不一定准确，结合之前提到对数据索引的理解来看，还记得嘛？就把索引当作有序序列来理解，反思 SQL。</p>
<p>&emsp;•    Set profiling , show profiles for query 操作<br>&emsp;&emsp;&emsp;执行开销<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    注意，有问题的 SQL 如果重复执行，可能在缓存里，这时要注意避免缓存影响。通过这里可以看到。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    执行时间超过 0.005 秒的频繁操作 SQL 建议都分析一下。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    深入理解数据库执行的过程和开销的分布<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        Show processlist<br>&emsp;•状态清单<br>&emsp;&emsp;&emsp;Sleep 状态， 通常代表资源未释放，如果是通过连接池，sleep 状态应该恒定在一定数量范围内<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    实战范例： 因前端数据输出时（特别是输出到用户终端）未及时关闭数据库连接，导致因网络连接速度产生大量 sleep 连接，在网速<br>出现异常时，数据库 too many connections 挂死。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    简单解读，数据查询和执行通常只需要不到 0.01 秒，而网络输出通常需要 1 秒左右甚至更长，原本数据连接在 0.01 秒即可释放，但是因为前端程序未执行 close 操作，直接输出结果，那么在结果未展现在用户桌面前，该数据库连接一直维持在 sleep 状态！<br>&emsp;&emsp;&emsp;    Waiting for net, reading from net, writing to net<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    偶尔出现无妨<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如大量出现，迅速检查数据库到前端的网络连接状态和流量<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    案例: 因外挂程序，内网数据库大量读取，内网使用的百兆交换迅速爆满，导致大量连接阻塞在 waiting for net，数据库连接过多崩溃<br>&emsp;&emsp;&emsp;    Locked 状态<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    有更新操作锁定<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    通常使用 innodb 可以很好的减少 locked 状态的产生，但是切记，更新操作要正确使用索引，即便是低频次更新操作也不能疏忽。如上影响结果集范例所示。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    在 myisam 的时代，locked 是很多高并发应用的噩梦。所以 mysql 官方也开始倾向于推荐 innodb。<br>&emsp;&emsp;&emsp;    Copy to tmp table<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    索引及现有结构无法涵盖查询条件，才会建立一个临时表来满足查询要求，产生巨大的恐怖的 i/o 压力。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    很可怕的搜索语句会导致这样的情况，如果是数据分析，或者半夜的周期数据清理任务，偶尔出现，可以允许。频繁出现务必优化之。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    Copy to tmp table 通常与连表查询有关，建议逐渐习惯不使用连表查询。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    实战范例：<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■ 某社区数据库阻塞，求救，经查，其服务器存在多个数据库应用和网站，其中一个不常用的小网站数据库产生了一个恐怖的copy to tmp table 操作，导致整个硬盘 i/o 和 cpu 压力超载。Kill 掉该操作一切恢复。<br>&emsp;&emsp;&emsp;    Sending data<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    Sending data 并不是发送数据，别被这个名字所欺骗，这是从物理磁盘获取数据的进程，如果你的影响结果集较多，那么就需要从不同的磁盘碎片去抽取数据，<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    偶尔出现该状态连接无碍。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    回到上面影响结果集的问题，一般而言，如果 sending data 连接过多，通常是某查询的影响结果集过大，也就是查询的索引项不够优化。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如果出现大量相似的 SQL 语句出现在 show proesslist 列表中，并且都处于 sending data 状态，优化查询索引，记住用影响结果集的思路去思考。<br>&emsp;&emsp;&emsp;    Freeing items<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    理论上这玩意不会出现很多。偶尔出现无碍<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如果大量出现，内存，硬盘可能已经出现问题。比如硬盘满或损坏。<br>&emsp;&emsp;&emsp;    Sorting for …<br>&emsp;&emsp;&emsp;&emsp;&emsp;• 和 Sending     data 类似，结果集过大，排序条件没有索引化，需要在内存里排序，甚至需要创建临时结构排序。<br>&emsp;&emsp;&emsp;    其他<br>&emsp;&emsp;&emsp;&emsp;&emsp;• 还有很多状态，遇到了，去查查资料。基本上我们遇到其他状态的阻塞较少，所以不关心。</p>
<p>##分析流程<br>&emsp;•    基本流程<br>&emsp;&emsp;&emsp;    详细了解问题状况<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    Too many connections 是常见表象，有很多种原因。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    索引损坏的情况在 innodb 情况下很少出现。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如出现其他情况应追溯日志和错误信息。<br>&emsp;&emsp;&emsp;    了解基本负载状况和运营状况<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    基本运营状况<br>&emsp;•        当前每秒读请求<br>&emsp;•        当前每秒写请求<br>&emsp;•        当前在线用户<br>&emsp;•        当前数据容量 </p>
<p><strong>基本负载情况 </strong><br>&emsp;•    学会使用这些指令<br>&emsp;&emsp;&emsp;    Top<br>&emsp;&emsp;&emsp;    Vmstat<br>&emsp;&emsp;&emsp;    uptime<br>&emsp;&emsp;&emsp;    iostat<br>&emsp;&emsp;&emsp;    df<br>&emsp;•    Cpu 负载构成<br>&emsp;&emsp;&emsp;    特别关注 i/o 压力( wa%)<br>&emsp;&emsp;&emsp;    多核负载分配<br>&emsp;•    内存占用<br>&emsp;&emsp;&emsp;    Swap 分区是否被侵占<br>&emsp;&emsp;&emsp;    如 Swap 分区被侵占，物理内存是否较多空闲<br>&emsp;•    磁盘状态<br>&emsp;&emsp;&emsp;    硬盘满和 inode 节点满的情况要迅速定位和迅速处理<br>&emsp;&emsp;&emsp;    了解具体连接状况  </p>
<p><strong>当前连接数</strong><br>&emsp;•    Netstat –an|grep 3306|wc –l<br>&emsp;•    Show processlist  </p>
<p><strong>当前连接分布 show processlist</strong><br>&emsp;•    前端应用请求数据库不要使用 root 帐号！<br>&emsp;&emsp;&emsp;    Root 帐号比其他普通帐号多一个连接数许可。<br>&emsp;&emsp;&emsp;    前端使用普通帐号，在 too many connections 的时候 root 帐号仍可以登录数据库查询 show processlist!<br>&emsp;&emsp;&emsp;    记住，前端应用程序不要设置一个不叫 root 的 root 帐号来糊弄！非 root 账户是骨子里的，而不是名义上的。<br>&emsp;•    状态分布<br>&emsp;&emsp;&emsp;    不同状态代表不同的问题，有不同的优化目标。<br>&emsp;&emsp;&emsp;    参见如上范例。<br>&emsp;•    雷同 SQL 的分布<br>&emsp;&emsp;&emsp;    是否较多雷同 SQL 出现在同一状态  </p>
<p><strong>当前是否有较多慢查询日志</strong><br>&emsp;•    是否锁定<br>&emsp;•    影响结果集<br>&emsp;&emsp;&emsp;    频繁度分析  </p>
<p><strong>写频繁度</strong><br>&emsp;•    如果 i/o 压力高，优先分析写入频繁度<br>&emsp;•    Mysqlbinlog 输出最新 binlog 文件，编写脚本拆分<br>&emsp;•    最多写入的数据表是哪个<br>&emsp;•    最多写入的数据 SQL 是什么<br>&emsp;•    是否存在基于同一主键的数据内容高频重复写入？<br>&emsp;•    涉及架构优化部分，参见架构优化-缓存异步更新<br><strong>读取频繁度</strong><br>&emsp;•    如果 cpu 资源较高，而 i/o 压力不高，优先分析读取频繁度<br>&emsp;•    程序中在封装的 db 类增加抽样日志即可，抽样比例酌情考虑，以不显著影响系统负载压力为底线。<br>&emsp;•    最多读取的数据表是哪个<br>&emsp;•    最多读取的数据 SQL 是什么<br>&emsp;&emsp;&emsp;    该 SQL 进行 explain 和 set profiling 判定<br>&emsp;&emsp;&emsp;    注意判定时需要避免 query cache 影响<br>&emsp;&emsp;&emsp;    比如，在这个 SQL 末尾增加一个条件子句 and 1=1 就可以<br>避免从 query cache 中获取数据，而得到真实的执行状态分析。<br>&emsp;• 是否存在同一个查询短期内频繁出现的情况<br>&emsp;&emsp;&emsp;    涉及前端缓存优化<br>&emsp;&emsp;&emsp;    抓大放小，解决显著问题 </p>
<p><strong>不苛求解决所有优化问题，但是应以保证线上服务稳定可靠为目标。</strong></p>
<p><strong>解决与评估要同时进行，新的策略或解决方案务必经过评估后上线。</strong></p>
<p>##总结<br>&emsp;•    要学会怎样分析问题，而不是单纯拍脑袋优化<br>&emsp;•    慢查询只是最基础的东西，要学会优化 0.01 秒的查询请求。<br>&emsp;•    当发生连接阻塞时，不同状态的阻塞有不同的原因，要找到原因，如果不对症下药，就会南辕北辙<br>&emsp;&emsp;&emsp;    范例：如果本身系统内存已经超载，已经使用到了 swap，而还在考虑加大缓存来优化查询，那就是自寻死路了。<br>&emsp;•    监测与跟踪要经常做，而不是出问题才做<br>&emsp;&emsp;&emsp;    读取频繁度抽样监测<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    全监测不要搞，i/o 吓死人。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    按照一个抽样比例抽样即可。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    针对抽样中发现的问题，可以按照特定 SQL 在特定时间内监测一段全查询记录，但仍要考虑 i/o 影响。<br>&emsp;&emsp;&emsp;    写入频繁度监测<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    基于 binlog 解开即可，可定时或不定时分析。<br>&emsp;&emsp;&emsp;    微慢查询抽样监测<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    高并发情况下，查询请求时间超过 0.01 秒甚至 0.005 秒的，建议酌情抽样记录。<br>&emsp;&emsp;&emsp;    连接数预警监测<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    连接数超过特定阈值的情况下，虽然数据库没有崩溃，建议记录相关连接状态。<br>&emsp;•    学会通过数据和监控发现问题，分析问题，而后解决问题顺理成章。特别是要学会在日常监控中发现隐患，而不是问题爆发了才去处理和解决。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Mysql-执行优化&quot;&gt;&lt;a href=&quot;#Mysql-执行优化&quot; class=&quot;headerlink&quot; title=&quot;Mysql 执行优化&quot;&gt;&lt;/a&gt;Mysql 执行优化&lt;/h1&gt;&lt;h2 id=&quot;认识数据索引&quot;&gt;&lt;a href=&quot;#认识数据索引&quot; class=&quot;
    
    </summary>
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Mysql 架构优化—mysql</title>
    <link href="http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_framework/"/>
    <id>http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_framework/</id>
    <published>2017-11-05T06:44:32.361Z</published>
    <updated>2017-11-05T07:38:26.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mysql-架构优化"><a href="#Mysql-架构优化" class="headerlink" title="Mysql 架构优化"></a>Mysql 架构优化</h1><h2 id="架构优化目标"><a href="#架构优化目标" class="headerlink" title="架构优化目标"></a>架构优化目标</h2><p>1). 防止单点隐患<br>&emsp;•    所谓单点隐患，就是某台设备出现故障，会导致整体系统的不可用，这个设备就是单点隐患。<br>&emsp;•    理解连带效应，所谓连带效应，就是一种问题会引发另一种故障，举例而言，<br>memcache+mysql 是一种常见缓存组合，在前端压力很大时，如果 memcache 崩溃，<br>理论上数据会通过 mysql 读取，不存在系统不可用情况，但是 mysql 无法对抗如此大的压力冲击，会因此连带崩溃。因 A 系统问题导致 B 系统崩溃的连带问题，在运维过程中会频繁出现。<br>&emsp;&emsp;&emsp;    实战范例： 在 mysql 连接不及时释放的应用环境里，当网络环境异常（同机房友邻服务器遭受拒绝服务攻击，出口阻塞），网络延迟加剧，空连接数急剧增加，导致数据库连接过多崩溃。<br>&emsp;&emsp;&emsp;    实战范例 2：前端代码 通常我们封装 mysql_connect 和 memcache_connect，二者的顺序不同，会产生不同的连带效应。如果 mysql_connect 在前，那么一旦 memcache 连接阻塞，会连带 mysql 空连接过多崩溃。<br>&emsp;&emsp;&emsp;    连带效应是常见的系统崩溃，日常分析崩溃原因的时候需要认真考虑连带效应的影响，头疼医头，脚疼医脚是不行的。  </p>
<p>2). 方便系统扩容<br>&emsp;•    数据容量增加后，要考虑能够将数据分布到不同的服务器上。<br>&emsp;•    请求压力增加时，要考虑将请求压力分布到不同服务器上。<br>&emsp;•    扩容设计时需要考虑防止单点隐患。  </p>
<p>3). 安全可控，成本可控<br>&emsp;•    数据安全，业务安全<br>&emsp;•    人力资源成本&gt;带宽流量成本&gt;硬件成本<br>&emsp;&emsp;&emsp;    成本与流量的关系曲线应低于线性增长（流量为横轴，成本为纵轴）。<br>&emsp;&emsp;&emsp;    规模优势<br>&emsp;•    本教程仅就与数据库有关部分讨论，与数据库无关部门请自行参阅其他学习资料。  </p>
<p>##分布式方案<br>1). 分库&amp;拆表方案<br>&emsp;•    基本认识<br>&emsp;&emsp;&emsp;    用分库&amp;拆表是解决数据库容量问题的唯一途径。<br>&emsp;&emsp;&emsp;    分库&amp;拆表也是解决性能压力的最优选择。<br>&emsp;&emsp;&emsp;    分库 – 不同的数据表放到不同的数据库服务器中（也可能是虚拟服务器）<br>&emsp;&emsp;&emsp;    拆表 – 一张数据表拆成多张数据表，可能位于同一台服务器，也可能位于多台服务器（含虚拟服务器）。  </p>
<p>&emsp;•    去关联化原则<br>&emsp;&emsp;&emsp;    摘除数据表之间的关联，是分库的基础工作。<br>&emsp;&emsp;&emsp;    摘除关联的目的是，当数据表分布到不同服务器时，查询请求容易分发和处理。<br>&emsp;&emsp;&emsp;    学会理解反范式数据结构设计，所谓反范式，第一要点是不用外键，不允许<br>Join 操作，不允许任何需要跨越两个表的查询请求。第二要点是适度冗余减少查询请求，比如说，信息表，fromuid, touid, message 字段外，还需要一个 fromuname 字段记录用户名，这样查询者通过 touid 查询后，能够立即得到发信人的用户名，而无需进行另一个数据表的查询。<br>&emsp;&emsp;&emsp;    去关联化处理会带来额外的考虑，比如说，某一个数据表内容的修改，对另一个数据表的影响。这一点需要在程序或其他途径去考虑。  </p>
<p>&emsp;•    分库方案<br>&emsp;&emsp;&emsp;    安全性拆分<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    将高安全性数据与低安全性数据分库，这样的好处第一是便于维护，第二是高安全性数据的数据库参数配置可以以安全优先，而低安全性数据的参数配置以性能优先。参见运维优化相关部分。<br>&emsp;&emsp;&emsp;    顺序写数据与随机读写数据分库<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        顺序数据与随机数据区分存储地址，保证物理 i/o 优化。这个实话说，我只听说了概念，还没学会怎么实践。<br>&emsp;&emsp;&emsp;    基于业务逻辑拆分<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    根据数据表的内容构成，业务逻辑拆分，便于日常维护和前端调用。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    基于业务逻辑拆分，可以减少前端应用请求发送到不同数据库服务器的频次，从而减少链接开销。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    基于业务逻辑拆分，可保留部分数据关联，前端 web 工程师可在限度范围内执行关联查询。<br>&emsp;&emsp;&emsp;    基于负载压力拆分<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    基于负载压力对数据结构拆分，便于直接将负载分担给不同的服务器。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    基于负载压力拆分，可能拆分后的数据库包含不同业务类型的数据表，日常维护会有一定的烦恼。  </p>
<p>&emsp;•    分表方案<br>&emsp;&emsp;&emsp;    数据量过大或者访问压力过大的数据表需要切分<br>&emsp;&emsp;&emsp;    忙闲分表<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    单数据表字段过多，可将频繁更新的整数数据与非频繁更新的字符串数据切分<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    范例 user 表 ，个人简介，地址，QQ 号，联系方式，头像 这些字段为字符串类型，更新请求少； 最后登录时间，在线时常，访问次数，信件数这些字段为整数型字段，更新频繁，可以将后面这些更新频繁的字段独立拆出一张数据表，表内容变少，索引结构变少，读写请求变快。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■    横向切表<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;● 等分切表，如哈希切表或其他基于对某数字取余的切表。等分切表的优点是负载很方便的分布到不同服务器；缺点是当容量继续增加时无法方便的扩容，需要重新进行数据的切分或转表。而且一些关键主键不易处理。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    递增切表，比如每 1kw 用户开一个新表，优点是可以适应数据的自增趋势；缺点是往往新数据负载高，压力分配不平均。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    日期切表，适用于日志记录式数据，优缺点等同于递增切表。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    个人倾向于递增切表，具体根据应用场景决定。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        热点数据分表<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    将数据量较大的数据表中将读写频繁的数据抽取出来，形成热点数据表。通常一个庞大数据表经常被读写的内容往往具有一定的集中性，如果这些集中数据单独处理，就会极大减少整体系统的负载。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    热点数据表与旧有数据关系<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>    可以是一张冗余表，即该表数据丢失不会妨碍使用，因源数据仍存在于旧有结构中。优点是安全性高，维护方便，缺点是写压力不能分担，仍需要同步写回原系统。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>    可以是非冗余表，即热点数据的内容原有结构不再保存，优点是读写效率全部优化；缺点是当热点数据发生变化时，维护量较大。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>    具体方案选择需要根据读写比例决定，在读频率远高于写频率情况下，优先考虑冗余表方案。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●        热点数据表可以用单独的优化的硬件存储，比如昂贵的闪存卡或大内存系统。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●        热点数据表的重要指标<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>        热点数据的定义需要根据业务模式自行制定策略，常见策略为，按照最新的操作时间；按照内容丰富度等等。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>        数据规模，比如从 1000 万条数据，抽取出 100 万条热点数据。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>        热点命中率，比如查询 10 次，多少次命中在热点数据内。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>        理论上，数据规模越小，热点命中率越高，说明效果越好。需要根据业务自行评估。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    热点数据表的动态维护<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>    加载热点数据方案选择<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>    定时从旧有数据结构中按照新的策略获取<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>    在从旧有数据结构读取时动态加载到热点数据<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;●    剔除热点数据方案选择<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>    基于特定策略，定时将热点数据中访问频次较少的数据剔除<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</em>    如热点数据是冗余表，则直接删除即可，如不是冗余表，需要回写给旧有数据结构。<br>&emsp;&emsp;通常，热点数据往往是基于缓存或者 key-value 方案冗余存储，所以这里提到的热点数据表，其实更多是理解思路，用到的场合可能并不多….  </p>
<p>&emsp;•    表结构设计<br>&emsp;&emsp;&emsp;查询冗余表设计<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        涉及分表操作后，一些常见的索引查询可能需要跨表，带来不必要的麻烦。 确认查询请求远大于写入请求时，应设置便于查询项的冗余表。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        实战范例，<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        用户分表，将用户库分成若干数据表<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        基于用户名的查询和基于 uid 的查询都是高并发请求。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■            用户分表基于 uid 分成数据表，同时基于用户名做对应冗余表。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆冗余表要点<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        数据一致性，简单说，同增，同删，同更新。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        可以做全冗余，或者只做主键关联的冗余，比如通过用户名查询 uid，再基于 uid 查询源表。<br>&emsp;&emsp;&emsp;    中间数据表<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        为了减少会涉及大规模影响结果集的表数据操作，比如 count，sum 操作。应将一些统计类数据通过中间数据表保存。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        中间数据表应能通过源数据表恢复。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        实战范例：<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        论坛板块的发帖量，回帖量，每日新增数据等<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        网站每日新增用户数等。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        后台可以通过源数据表更新该数字。<br>&emsp;&emsp;&emsp;    历史数据表<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        历史数据表对应于热点数据表，将需求较少又不能丢弃的数据存入，仅在少数情况下被访问。  </p>
<p>2). 主从架构<br>&emsp;•    基本认识<br>&emsp;&emsp;&emsp;    读写分离对负载的减轻远远不如分库分表来的直接。<br>&emsp;&emsp;&emsp;    写压力会传递给从表，只读从库一样有写压力，一样会产生读写锁！<br>&emsp;&emsp;&emsp;    一主多从结构下，主库是单点隐患，很难解决（如主库当机，从库可以响应读写，但是无法自动担当主库的分发功能）<br>&emsp;&emsp;&emsp;    主从延迟也是重大问题。一旦有较大写入问题，如表结构更新，主从会产生巨大延迟。  </p>
<p>&emsp;•    应用场景<br>&emsp;&emsp;&emsp;    在线热备<br>&emsp;&emsp;&emsp;    异地分布<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    写分布，读统一。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    仍然困难重重，受限于网络环境问题巨多！<br>&emsp;&emsp;&emsp;    自动障碍转移<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    主崩溃，从自动接管<br>&emsp;&emsp;&emsp;    个人建议，负载均衡主要使用分库方案，主从主要用于热备和障碍转移。 </p>
<p>&emsp;•    潜在优化点<br>&emsp;&emsp;&emsp;    为了减少写压力，有些人建议主不建索引提升 i/o 性能，从建立索引满足查询要求。个人认为这样维护较为麻烦。而且从本身会继承主的 i/o 压力，因此优化价值有限。该思路特此分享，不做推荐。  </p>
<p>3). 故障转移处理<br>&emsp;•        要点<br>&emsp;&emsp;&emsp;    程序与数据库的连接，基于虚地址而非真实 ip，由负载均衡系统监控。<br>&emsp;&emsp;&emsp;    保持主从结构的简单化，否则很难做到故障点摘除。  </p>
<p>&emsp;•        思考方式<br>&emsp;&emsp;&emsp;    遍历对服务器集群的任何一台服务器，前端 web，中间件，监控，缓存，db 等等，假设该服务器出现故障，系统是否会出现异常？用户访问是否会出现异常。<br>&emsp;&emsp;&emsp;    目标：任意一台服务器崩溃，负载和数据操作均会很短时间内自动转移到其他服务器，不会影响业务的正常进行。不会造成恶性的数据丢失。（哪些是可以丢失的，哪些是不能丢失的）  </p>
<h2 id="缓存方案"><a href="#缓存方案" class="headerlink" title="缓存方案"></a>缓存方案</h2><p>1). 缓存结合数据库的读取<br>&emsp;•        Memcached/redis 是最常用的缓存系统<br>&emsp;•        Mysql 最新版本已经开始支持 memcache 插件，但据牛人分析，尚不成熟，暂不推荐。<br>&emsp;•        数据读取<br>&emsp;&emsp;&emsp;    并不是所有数据都适合被缓存，也并不是进入了缓存就意味着效率提升。<br>&emsp;&emsp;&emsp;    命中率是第一要评估的数据。<br>&emsp;&emsp;&emsp;    如何评估进入缓存的数据规模，以及命中率优化，是非常需要细心分析的。<br>&emsp;•    实景分析： 前端请求先连接缓存，缓存未命中连接数据库，进行查询，未命中状态比单纯连接数据库查询多了一次连接和查询的操作；如果缓存命中率很低，则这个额外的操作非但不能提高查询效率，反而为系统带来了额外的负载和复杂性，得不偿失。<br>&emsp;&emsp;&emsp;    相关评估类似于热点数据表的介绍。<br>&emsp;&emsp;&emsp;    善于利用内存，请注意数据存储的格式及压缩算法。<br>&emsp;•    Key-value 方案繁多，本培训文档暂不展开。  </p>
<p>2). 缓存结合数据库的写入<br>&emsp;•    利用缓存不但可以减少数据读取请求，还可以减少数据库写入 i/o 压力<br>&emsp;•    缓存实时更新，数据库异步更新<br>&emsp;&emsp;&emsp;    缓存实时更新数据，并将更新记录写入队列<br>&emsp;&emsp;&emsp;    可以使用类似 mq 的队列产品，自行建立队列请注意使用 increment 来维持队列序号。<br>&emsp;&emsp;&emsp;    不建议使用 get 后处理数据再 set 的方式维护队列<br>&emsp;•    测试范例：<br>&emsp;•    范例 1</p>
<pre><code>$var=Memcache_get($memcon,”var”);  
 $var++;  
memcache_set($memcon,”var”,$var);  
</code></pre><p>&emsp;这样一个脚本，使用 apache ab 去跑，100 个并发，跑 10000 次，然后输出缓存存取的数据，很遗憾，并不是 1000，而是 5000 多，6000 多这样的数字，中间的数字全在 get &amp; set 的过程中丢掉了。<br>原因，读写间隔中其他并发写入，导致数据丢失。<br>&emsp;•    范例 2<br>&emsp;用 memcache_increment 来做这个操作，同样跑测试会得到完整的 10000，一条数据不会丢。<br>&emsp;•    结论： 用 increment 存储队列编号，用标记+编号作为 key 存储队列内容。<br>&emsp;&emsp;&emsp;    后台基于缓存队列读取更新数据并更新数据库<br>&emsp;•    基于队列读取后可以合并更新<br>&emsp;•    更新合并率是重要指标<br>&emsp;•    实战范例：<br>某论坛热门贴，前端不断有 views=views+1 数据更新请求。<br>缓存实时更新该状态后台任务对数据库做异步更新时，假设执行周期是 5 分钟，那么五分钟可能<br>会接收到这样的请求多达数十次乃至数百次，合并更新后只执行一次 update 即可。<br>类似操作还包括游戏打怪，生命和经验的变化；个人主页访问次数的变化等。<br>&emsp;&emsp;&emsp;    异步更新风险<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    前后端同时写，可能导致覆盖风险。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    使用后端异步更新，则前端应用程序就不要写数据库，否则可能造成写入冲突。一种兼容的解决方案是，前端和后端不要写相同的字段。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆    实战范例：<br>&emsp;&emsp;&emsp;&emsp;&emsp;用户在线上时，后台异步更新用户状态。管理员后台屏蔽用户是直接更新数据库。<br>结果管理员屏蔽某用户操作完成后，因该用户在线有操作，后台异步更新程序再次基于缓存更新用户状态，用户状态被复活，屏蔽失效。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        缓存数据丢失或服务崩溃可能导致数据丢失风险。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        如缓存中间出现故障，则缓存队列数据不会回写到数据库，而用户会认为已经完成，此时会带来比较明显的用户体验问题。<br>&emsp;&emsp;&emsp;&emsp;&emsp;◆        一个不彻底的解决方案是，确保高安全性，高重要性数据实时数据更新，而低安全性数据通过缓存异步回写方式完成。此外，使用相对数值操作而不是绝对数值操作更安全。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        范例：支付信息，道具的购买与获得，一旦丢失会对用户造成极大的伤害。而经验值，访问数字，如果只丢失了很少时间的内容，用户还是可以容忍的。<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;■        范例：如果使用 Views=Views+…的操作，一旦出现数据格式错误，从 binlog 中反推是可以进行数据还原，但是如果使用 Views=特定值的操作，一旦缓存中数据有错误，则直接被赋予了一个错误数据，无法回溯！<br>&emsp;&emsp;&emsp;    异步更新如出现队列阻塞可能导致数据丢失风险。<br>&emsp;&emsp;&emsp;    异步更新通常是使用缓存队列后，在后台由 cron 或其他守护进程写入数据库。<br>如果队列生成的速度&gt;后台更新写入数据库的速度，就会产生阻塞，导致数据越累计越多，数据库响应迟缓，而缓存队列无法迅速执行，导致溢出或者过期失效。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Mysql-架构优化&quot;&gt;&lt;a href=&quot;#Mysql-架构优化&quot; class=&quot;headerlink&quot; title=&quot;Mysql 架构优化&quot;&gt;&lt;/a&gt;Mysql 架构优化&lt;/h1&gt;&lt;h2 id=&quot;架构优化目标&quot;&gt;&lt;a href=&quot;#架构优化目标&quot; class=&quot;
    
    </summary>
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Mysql 运维优化—mysql</title>
    <link href="http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_ops/"/>
    <id>http://zhaoxj0217.github.io/2017/11/05/mysql_optimize_ops/</id>
    <published>2017-11-05T06:27:40.844Z</published>
    <updated>2017-11-05T06:42:40.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mysql-运维优化"><a href="#Mysql-运维优化" class="headerlink" title="Mysql 运维优化"></a>Mysql 运维优化</h1><h2 id="存储引擎类型"><a href="#存储引擎类型" class="headerlink" title="存储引擎类型"></a>存储引擎类型</h2><p>&emsp;•    Myisam 速度快，响应快。表级锁是致命问题。<br>&emsp;•    Innodb 目前主流存储引擎<br>&emsp;&emsp;&emsp;•    行级锁<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    务必注意影响结果集的定义是什么<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    行级锁会带来更新的额外开销，但是通常情况下是值得的。<br>&emsp;&emsp;&emsp;•    事务提交<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    对 i/o 效率提升的考虑<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    对安全性的考虑<br>&emsp;•    HEAP 内存引擎<br>&emsp;&emsp;&emsp;•    频繁更新和海量读取情况下仍会存在锁定状况  </p>
<h2 id="内存使用考量"><a href="#内存使用考量" class="headerlink" title="内存使用考量"></a>内存使用考量</h2><p>&emsp;•    理论上，内存越大，越多数据读取发生在内存，效率越高<br>&emsp;•    要考虑到现实的硬件资源和瓶颈分布<br>&emsp;•    学会理解热点数据，并将热点数据尽可能内存化<br>&emsp;&emsp;&emsp;•    所谓热点数据，就是最多被访问的数据。<br>&emsp;&emsp;&emsp;•    通常数据库访问是不平均的，少数数据被频繁读写，而更多数据鲜有读写。<br>&emsp;&emsp;&emsp;•    学会制定不同的热点数据规则，并测算指标。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    热点数据规模，理论上，热点数据越少越好，这样可以更好的满足业务的增长趋势。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    响应满足度，对响应的满足率越高越好。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    比如依据最后更新时间，总访问量，回访次数等指标定义热点数据，并测算不同定义模式下的热点数据规模  </p>
<h2 id="性能与安全性考量"><a href="#性能与安全性考量" class="headerlink" title="性能与安全性考量"></a>性能与安全性考量</h2><p>&emsp;•    数据提交方式<br>&emsp;&emsp;&emsp;•    innodb_flush_log_at_trx_commit = 1 每次自动提交，安全性高，i/o 压力大<br>&emsp;&emsp;&emsp;•    innodb_flush_log_at_trx_commit = 2 每秒自动提交，安全性略有影响，i/o 承载强。<br>&emsp;•    日志同步<br>&emsp;&emsp;&emsp;•    Sync-binlog    =1 每条自动更新，安全性高，i/o 压力大<br>&emsp;&emsp;&emsp;•    Sync-binlog = 0 根据缓存设置情况自动更新，存在丢失数据和同步延迟风险，i/o 承载力强。<br>&emsp;•    性能与安全本身存在相悖的情况，需要在业务诉求层面决定取舍<br>&emsp;&emsp;&emsp;•    学会区分什么场合侧重性能，什么场合侧重安全<br>&emsp;&emsp;&emsp;•    学会将不同安全等级的数据库用不同策略管理  </p>
<h2 id="存储压力优化"><a href="#存储压力优化" class="headerlink" title="存储压力优化"></a>存储压力优化</h2><p>&emsp;•    顺序读写性能远高于随机读写<br>&emsp;•    日志类数据可以使用顺序读写方式进行<br>&emsp;•    将顺序写数据和随机读写数据分成不同的物理磁盘，有助于 i/o 压力的疏解，前提是，你确信你的 i/o 压力主要来自于可顺序写操作（因随机读写干扰导致不能顺序写，但是确实可以用顺序写方式进行的 i/o 操作）。  </p>
<h2 id="运维监控体系"><a href="#运维监控体系" class="headerlink" title="运维监控体系"></a>运维监控体系</h2><p>&emsp;•    系统监控<br>&emsp;&emsp;&emsp;•    服务器资源监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    Cpu, 内存，硬盘空间，i/o 压力<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    设置阈值报警<br>&emsp;&emsp;&emsp;•    服务器流量监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    外网流量，内网流量<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    设置阈值报警<br>&emsp;&emsp;&emsp;•    连接状态监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    Show processlist 设置阈值，每分钟监测，超过阈值记录<br>&emsp;•    应用监控<br>&emsp;&emsp;&emsp;•    慢查询监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    慢查询日志<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    如果存在多台数据库服务器，应有汇总查阅机制。<br>&emsp;&emsp;&emsp;•    请求错误监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    高频繁应用中，会出现偶发性数据库连接错误或执行错误，将错误信息记录到日志，查看每日的比例变化。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    偶发性错误，如果数量极少，可以不用处理，但是需时常监控其趋势。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    会存在恶意输入内容，输入边界限定缺乏导致执行出错，需基于此防止恶意入侵探测行为。<br>&emsp;&emsp;&emsp;•    微慢查询监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    高并发环境里，超过 0.01 秒的查询请求都应该关注一下。<br>&emsp;&emsp;&emsp;•    频繁度监控<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    写操作，基于 binlog，定期分析。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    读操作，在前端 db 封装代码中增加抽样日志，并输出执行时间。<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    分析请求频繁度是开发架构 进一步优化的基础<br>&emsp;&emsp;&emsp;&emsp;&emsp;•    最好的优化就是减少请求次数！</p>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>&emsp;&emsp;&emsp;•    监控与数据分析是一切优化的基础。<br>&emsp;&emsp;&emsp;•    没有运营数据监测就不要妄谈优化！<br>监控要注意不要产生太多额外的负载，不要因监控带来太多额外系统开销  </p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Mysql-运维优化&quot;&gt;&lt;a href=&quot;#Mysql-运维优化&quot; class=&quot;headerlink&quot; title=&quot;Mysql 运维优化&quot;&gt;&lt;/a&gt;Mysql 运维优化&lt;/h1&gt;&lt;h2 id=&quot;存储引擎类型&quot;&gt;&lt;a href=&quot;#存储引擎类型&quot; class=&quot;
    
    </summary>
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhaoxj0217.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>十条命令在一分钟内对机器性能问题进行诊断—linux</title>
    <link href="http://zhaoxj0217.github.io/2017/11/05/linux_10_command/"/>
    <id>http://zhaoxj0217.github.io/2017/11/05/linux_10_command/</id>
    <published>2017-11-05T02:52:01.949Z</published>
    <updated>2017-11-05T03:18:25.875Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&lt;转&gt;如果你的Linux服务器突然负载暴增，告警短信快发爆你的手机，如何在最短时间内找出Linux性能问题所在？来看Netflix性能工程团队的这篇博文，看它们通过十条命令在一分钟内对机器性能问题进行诊断。<br>概述<br>通过执行以下命令，可以在1分钟内对系统资源使用情况有个大致的了解。<br>&emsp;•    uptime<br>&emsp;•    dmesg | tail<br>&emsp;•    vmstat 1<br>&emsp;•    mpstat -P ALL 1<br>&emsp;•    pidstat 1<br>&emsp;•    iostat -xz 1<br>&emsp;•    free -m<br>&emsp;•    sar -n DEV 1<br>&emsp;•    sar -n TCP,ETCP 1<br>&emsp;•    top<br>&emsp;其中一些命令需要安装sysstat包，有一些由procps包提供。这些命令的输出，有助于快速定位性能瓶颈，检查出所有资源（CPU、内存、磁盘IO等）的利用率（utilization）、饱和度（saturation）和错误（error）度量，也就是所谓的USE方法。<br>&emsp;下面我们来逐一介绍下这些命令，有关这些命令更多的参数和说明，请参照命令的手册。</p>
<p>&emsp;<strong>uptime</strong> </p>
<pre><code>$ uptime  
23:51:26 up 21:31,  1 user,  load average: 30.02, 26.43, 19.02
</code></pre><p>&emsp;这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。<br>&emsp;命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是区域缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。<br>&emsp;上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。</p>
<p>&emsp;<strong>dmesg | tail</strong></p>
<pre><code>$ dmesg | tail  
[1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0
[...]
[1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child
[1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB
[2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request.  Check SNMP counters.
</code></pre><p>&emsp;该命令会输出系统日志的最后10行。示例中的输出，可以看见一次内核的oom kill和一次TCP丢包。这些日志可以帮助排查性能问题。千万不要忘了这一步。</p>
<p>&emsp;<strong>vmstat 1</strong>   </p>
<pre><code>$ vmstat 1
procs ---------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b swpd   free   buff  cache   si   sobibo   in   cs us sy id wa st
34  00 200889792  73708 59182800 0 56   10 96  1  3  0  0
32  00 200889920  73708 59186000 0   592 13284 4282 98  1  1  0  0
32  00 200890112  73708 59186000 0 0 9501 2154 99  1  0  0  0
32  00 200889568  73712 59185600 048 11900 2459 99  0  0  0  0
32  00 200890208  73712 59186000 0 0 15898 4840 98  1  1  0  0
^C
</code></pre><p>vmstat(8) 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列：<br>&emsp;•    r：等待在CPU资源的进程数。这个数据比平均负载更加能够体现CPU负载情况，数据中不包含等待IO的进程。如果这个数值大于机器CPU核数，那么机器的CPU资源已经饱和。<br>&emsp;•    free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的free命令，可以更详细的了解系统内存的使用情况。<br>&emsp;•    si, so：交换区写入和读取的数量。如果这个数据不为0，说明系统已经在使用交换区（swap），机器物理内存已经不足。<br>&emsp;•    us, sy, id, wa, st：这些都代表了CPU时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。<br>上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。一般情况下，如果用户时间和系统时间相加非常大，CPU出于忙于执行指令。如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。<br>示例命令的输出可以看见，大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。 </p>
<p>&emsp;<strong>mpstat -P ALL 1</strong>  </p>
<pre><code>$ mpstat -P ALL 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)
07:38:49 PM  CPU   %usr  %nice   %sys %iowait   %irq  %soft  %steal  %guest  %gnice  %idle
07:38:50 PM  all  98.47   0.00   0.750.00   0.00   0.000.000.000.00   0.78
07:38:50 PM0  96.04   0.00   2.970.00   0.00   0.000.000.000.00   0.99
07:38:50 PM1  97.00   0.00   1.000.00   0.00   0.000.000.000.00   2.00
07:38:50 PM2  98.00   0.00   1.000.00   0.00   0.000.000.000.00   1.00
07:38:50 PM3  96.97   0.00   0.000.00   0.00   0.000.000.000.00   3.03
[...]
</code></pre><p>&emsp;该命令可以显示每个CPU的占用情况，如果有一个CPU占用率特别高，那么有可能是一个单线程应用程序引起的。<br>pidstat 1   </p>
<pre><code>$ pidstat 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015_x86_64_(32 CPU)
07:41:02 PM   UID   PID%usr %system  %guest%CPU   CPU  Command
07:41:03 PM 0 90.000.940.000.94 1  rcuos/0
07:41:03 PM 0  42145.665.660.00   11.3215  mesos-slave
07:41:03 PM 0  43540.940.940.001.89 8  java
07:41:03 PM 0  6521 1596.231.890.00 1598.1127  java
07:41:03 PM 0  6564 1571.707.550.00 1579.2528  java
07:41:03 PM 60004 601540.944.720.005.66 9  pidstat
07:41:03 PM   UID   PID%usr %system  %guest%CPU   CPU  Command
07:41:04 PM 0  42146.002.000.008.0015  mesos-slave
07:41:04 PM 0  6521 1590.001.000.00 1591.0027  java
07:41:04 PM 0  6564 1573.00   10.000.00 1583.0028  java
07:41:04 PM   108  67181.000.000.001.00 0  snmp-pass
07:41:04 PM 60004 601541.004.000.005.00 9  pidstat
^C
</code></pre><p>&emsp;pidstat命令输出进程的CPU占用率，该命令会持续输出，并且不会覆盖之前的数据，可以方便观察系统动态。如上的输出，可以看见两个JAVA进程占用了将近1600%的CPU时间，既消耗了大约16个CPU核心的运算资源。</p>
<p>&emsp;<strong>iostat -xz 1</strong>   </p>
<pre><code>$ iostat -xz 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
  73.960.003.730.030.06   22.21
Device:   rrqm/s   wrqm/s r/s w/srkB/swkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvda0.00 0.230.210.18 4.52 2.0834.37 0.009.98   13.805.42   2.44   0.09
xvdb0.01 0.001.028.94   127.97   598.53   145.79 0.000.431.780.28   0.25   0.25
xvdc0.01 0.001.028.86   127.79   595.94   146.50 0.000.451.820.30   0.27   0.26
dm-00.00 0.000.692.3210.4731.6928.01 0.013.230.713.98   0.13   0.04
dm-10.00 0.000.000.94 0.01 3.78 8.00 0.33  345.840.04  346.81   0.01   0.00
dm-20.00 0.000.090.07 1.35 0.3622.50 0.002.550.235.62   1.78   0.03
[...]
^C
</code></pre><p>&emsp;iostat命令主要用于查看机器磁盘IO情况。该命令输出的列，主要含义是：<br>&emsp;•    r/s, w/s, rkB/s, wkB/s：分别表示每秒读写次数和每秒读写数据量（千字节）。读写量过大，可能会引起性能问题。<br>&emsp;•    await：IO操作的平均等待时间，单位是毫秒。这是应用程序在和磁盘交互时，需要消耗的时间，包括IO等待和实际操作的耗时。如果这个数值过大，可能是硬件设备遇到了瓶颈或者出现故障。<br>&emsp;•    avgqu-sz：向设备发出的请求平均数量。如果这个数值大于1，可能是硬件设备已经饱和（部分前端硬件设备支持并行写入）。<br>&emsp;•    %util：设备利用率。这个数值表示设备的繁忙程度，经验值是如果超过60，可能会影响IO性能（可以参照IO操作平均等待时间）。如果到达100%，说明硬件设备已经饱和。<br>如果显示的是逻辑设备的数据，那么设备利用率不代表后端实际的硬件设备已经饱和。值得注意的是，即使IO性能不理想，也不一定意味这应用程序性能会不好，可以利用诸如预读取、写缓存等策略提升应用性能。</p>
<p>&emsp;<strong>free –m</strong>   </p>
<pre><code>$ free -m
 total   used   free sharedbuffers cached
Mem:245998  24545 221453 83 59541
-/+ buffers/cache:  23944 222053
Swap:0  0  0
</code></pre><p>&emsp;free命令可以查看系统内存的使用情况，-m参数表示按照兆字节展示。最后两列分别表示用于IO缓存的内存数，和用于文件系统页缓存的内存数。需要注意的是，第二行-/+ buffers/cache，看上去缓存占用了大量内存空间。这是Linux系统的内存使用策略，尽可能的利用内存，如果应用程序需要内存，这部分内存会立即被回收并分配给应用程序。因此，这部分内存一般也被当成是可用内存。<br>&emsp;如果可用内存非常少，系统可能会动用交换区（如果配置了的话），这样会增加IO开销（可以在iostat命令中提现），降低系统性能。</p>
<p>&emsp;<strong>sar -n DEV 1</strong> </p>
<pre><code>$ sar -n DEV 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015 _x86_64_(32 CPU)
12:16:48 AM IFACE   rxpck/s   txpck/srxkB/stxkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:49 AM  eth0  18763.00   5032.00  20686.42478.30  0.00  0.00  0.00  0.00
12:16:49 AMlo 14.00 14.00  1.36  1.36  0.00  0.00  0.00  0.00
12:16:49 AM   docker0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
12:16:49 AM IFACE   rxpck/s   txpck/srxkB/stxkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:50 AM  eth0  19763.00   5101.00  21999.10482.56  0.00  0.00  0.00  0.00
12:16:50 AMlo 20.00 20.00  3.25  3.25  0.00  0.00  0.00  0.00
12:16:50 AM   docker0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00
^C
</code></pre><p>&emsp;sar命令在这里可以查看网络设备的吞吐率。在排查性能问题时，可以通过网络设备的吞吐量，判断网络设备是否已经饱和。如示例输出中，eth0网卡设备，吞吐率大概在22 Mbytes/s，既176 Mbits/sec，没有达到1Gbit/sec的硬件上限。</p>
<p>&emsp;<strong>sar -n TCP,ETCP 1</strong> </p>
<pre><code>$ sar -n TCP,ETCP 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015_x86_64_(32 CPU)
12:17:19 AM  active/s passive/siseg/soseg/s
12:17:20 AM  1.00  0.00  10233.00  18846.00
12:17:19 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:20 AM  0.00  0.00  0.00  0.00  0.00
12:17:20 AM  active/s passive/siseg/soseg/s
12:17:21 AM  1.00  0.00   8359.00   6039.00
12:17:20 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:21 AM  0.00  0.00  0.00  0.00  0.00
^C
</code></pre><p>&emsp;sar命令在这里用于查看TCP连接状态，其中包括：<br>&emsp;•    active/s：每秒本地发起的TCP连接数，既通过connect调用创建的TCP连接；<br>&emsp;•    passive/s：每秒远程发起的TCP连接数，即通过accept调用创建的TCP连接；<br>&emsp;•    retrans/s：每秒TCP重传数量；<br>TCP连接数可以用来判断性能问题是否由于建立了过多的连接，进一步可以判断是主动发起的连接，还是被动接受的连接。TCP重传可能是因为网络环境恶劣，或者服务器压力过大导致丢包。  </p>
<p>&emsp;<strong>top</strong></p>
<pre><code>$ top  
top - 00:15:40 up 21:56,  1 user,  load average: 31.09, 29.87, 29.92
Tasks: 871 total,   1 running, 868 sleeping,   0 stopped,   2 zombie
%Cpu(s): 96.8 us,  0.4 sy,  0.0 ni,  2.7 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  25190241+total, 24921688 used, 22698073+free,60448 buffers
KiB Swap:0 total,0 used,0 free.   554208 cached Mem
   PID USER  PR  NIVIRTRESSHR S  %CPU %MEM TIME+ COMMAND
 20248 root  20   0  0.227t 0.012t  18748 S  3090  5.2  29812:58 java
  4213 root  20   0 2722544  64640  44232 S  23.5  0.0 233:35.37 mesos-slave
 66128 titancl+  20   0   24344   2332   1172 R   1.0  0.0   0:00.07 top
  5235 root  20   0 38.227g 547004  49996 S   0.7  0.2   2:02.74 java
  4299 root  20   0 20.015g 2.682g  16836 S   0.3  1.1  33:14.42 java
 1 root  20   0   33620   2920   1496 S   0.0  0.0   0:03.82 init
 2 root  20   0   0  0  0 S   0.0  0.0   0:00.02 kthreadd
 3 root  20   0   0  0  0 S   0.0  0.0   0:05.35 ksoftirqd/0
 5 root   0 -20   0  0  0 S   0.0  0.0   0:00.00 kworker/0:0H
 6 root  20   0   0  0  0 S   0.0  0.0   0:06.94 kworker/u256:0
 8 root  20   0   0  0  0 S   0.0  0.0   2:38.05 rcu_sched
</code></pre><p>&emsp;top命令包含了前面好几个命令的检查的内容。比如系统负载情况（uptime）、系统内存使用情况（free）、系统CPU使用情况（vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。同时，top命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU占用率最高的进程等。<br>&emsp;但是，top命令相对于前面一些命令，输出是一个瞬间值，如果不持续盯着，可能会错过一些线索。这时可能需要暂停top命令刷新，来记录和比对数据。  </p>
<p>&emsp;<strong>总结</strong><br>&emsp;排查Linux服务器性能问题还有很多工具，上面介绍的一些命令，可以帮助我们快速的定位问题。例如前面的示例输出，多个证据证明有JAVA进程占用了大量CPU资源，之后的性能调优就可以针对应用程序进行。 </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;lt;转&amp;gt;如果你的Linux服务器突然负载暴增，告警短信快发爆你的手机，如何在最短时间内找出Linux性能问题所在？来看Netflix性能工程团队的这篇博文，看它们通过十条命令在一分钟内对机器性能问题进行诊断。&lt;br&gt;概述&lt;br&gt;通过执行以下命令，可以在
    
    </summary>
    
      <category term="linux" scheme="http://zhaoxj0217.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://zhaoxj0217.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>基于tensorflow的‘端到端’的字符型验证码识别—cnn</title>
    <link href="http://zhaoxj0217.github.io/2017/11/02/captcha_by_cnn/"/>
    <id>http://zhaoxj0217.github.io/2017/11/02/captcha_by_cnn/</id>
    <published>2017-11-02T06:19:58.465Z</published>
    <updated>2017-11-05T06:23:02.320Z</updated>
    
    <content type="html"><![CDATA[<p>因有个验证码识别的需求，本来打算用SVM来识别验证码，但在查询资料的过程中看到了@斗大的熊猫<br><a href="http://blog.topspeedsnail.com/archives/10858" target="_blank" rel="external">TensorFlow练习20: 使用深度学习破解字符验证码</a><br>这篇文章<br>在github上也找到了基于这篇文章的代码整合<br><a href="https://github.com/zhengwh/captcha-tensorflow" target="_blank" rel="external">基于tensorflow的‘端到端’的字符型验证码识别</a></p>
<p>因为传统的机器学习方法，对于多位字符验证码都是采用的化整为零的方法：先分割成最小单位，再分别识别，然后再统一。同时还需对图片进行去躁，去干扰线等预处理，预处理的好坏直接影响识别率，所以想看下<br>卷积神经网络方法，是否真的更加通用简单，其他相关的说明都在这2个连接中</p>
<p>本文的代码是在win7,python35环境下跑的<br>针对captcha-tensorflow的代码做了一些修改（主要是数据读取的方式的修改，原来的代码是直接代码生成验证码图片进行训练，本文需要训练的验证码是通过其他方式批量生成后再进行训练）</p>
<p>修改后的代码目录与原来代码目录基本一致（去掉了没使用的验证码生成）</p>
<pre><code>capt
    __init__.py
    cfg.py  配置信息文件
    cnn_sys.py  CNN网络结构
    data_iter.py 可迭代的数据集
    predict.py 加载训练好的模型，然后对输入的图片进行预测
    train.py 对模型进行训练
    utils.py 一些公共使用的方法
tmp
work
</code></pre><p>cfg.py就根据实际情况配置下<br>cnn_sys.py 需要根据你视图验证码的图像大小，做下计算参数上的小调整<br>predict.py 根据需要修改获取验证数据方式<br>train.py  有个须知的地方：</p>
<pre><code># 从0开始训练数据
# sess.run(tf.global_variables_initializer())

#在训练一段时间后接着上次的训练
saver.restore(sess, tf.train.latest_checkpoint(model_path))
</code></pre><p>data_iter.py 提供数据与原来的方法有所区别:  </p>
<pre><code>&quot;&quot;&quot;
数据生成器
&quot;&quot;&quot;
import numpy as np

from capt.cfg import IMAGE_HEIGHT, IMAGE_WIDTH, CHAR_SET_LEN, MAX_CAPTCHA,trainSpace,testSpace,verifySpace
from capt.utils import convert2gray, text2vec
from tensorflow.python.platform import gfile
import os.path
from PIL import Image
import random
from os.path import join

no1 = 0
no2 = 1
dataSet = []
testdataSet = []
verifydataSet =[]

#从相应文件夹内读取验证码图片
def create_data_list(image_dir):
    if not gfile.Exists(image_dir):
        print(&quot;Image director &apos;&quot; + image_dir + &quot;&apos; not found.&quot;)
        return None

      extensions = [&apos;jpg&apos;]
      print(&quot;Looking for images in &apos;&quot; + image_dir + &quot;&apos;&quot;)
      file_list = []
      for extension in extensions:
        file_glob = os.path.join(image_dir, &apos;*.&apos; + extension)
        file_list.extend(gfile.Glob(file_glob))
      if not file_list:
        print(&quot;No files found in &apos;&quot; + image_dir + &quot;&apos;&quot;)
        return None

      imageList = []
      for file_name in file_list:
        images = []
        image = Image.open(file_name)
        img_raw =  np.array(image)
        image.close()
        label_name = os.path.basename(file_name).split(&apos;_&apos;)[0]
        images.append(img_raw)
        images.append(label_name)
        imageList.append(images)
      print(&quot;create imgList finish!&quot;)
      return imageList


#生成训练数据
def get_train_batch(batch_size=128):
       #因为待训练的图片很多（几十万张）一次读取很占内存，将他们分成1万一个子文件夹
    batch_x = np.zeros([batch_size, IMAGE_HEIGHT * IMAGE_WIDTH])
    batch_y = np.zeros([batch_size, MAX_CAPTCHA * CHAR_SET_LEN])

    global no1
    no1= no1+1
    print(&apos;no1&apos;,no1)
    global no2
    global dataSet
    image_dir = join(trainSpace, no2)
    if(no1&gt;n):#单个子集训练n个step
           no1=0
        no2 =no2+1
        if (no2 &gt; j):#全部子集训练完后循环训练
               no2 = 1
        image_dir = join(trainSpace, no2)
        dataSet = create_data_list(image_dir)
        print(&quot;create trainSET&quot;)
        print(&apos;image_dir&apos;, image_dir)
    print(&apos;no2&apos;, no2)
    if(not dataSet):
        dataSet = create_data_list(image_dir)
        print(&quot;create trainSET&quot;)
        print(&apos;image_dir&apos;, image_dir)

    for i in range(batch_size):
        data = dataSet[random.randint(0, len(dataSet)-1)]#随机读取数据
        image = data[0]
        text = data[1]
        image = convert2gray(image)
        batch_x[i, :] = image.flatten() / 255  # (image.flatten()-128)/128  mean为0
        batch_y[i, :] = text2vec(text)

    return batch_x, batch_y

#获取测试数据
def get_test_batch(batch_size=128):

    batch_x = np.zeros([batch_size, IMAGE_HEIGHT * IMAGE_WIDTH])
    batch_y = np.zeros([batch_size, MAX_CAPTCHA * CHAR_SET_LEN])

    global testdataSet
    if(not testdataSet):
           image_dir = testSpace
        testdataSet = create_data_list(image_dir)
        print(&quot;create testSET&quot;)

    for i in range(batch_size):
        data = testdataSet[random.randint(0, len(testdataSet)-1)]
        image = data[0]
        text = data[1]
        image = convert2gray(image)
        batch_x[i, :] = image.flatten() / 255  # (image.flatten()-128)/128  mean为0
        batch_y[i, :] = text2vec(text)

    return batch_x, batch_y

#获取验证数据
def get_verify():
    global verifydataSet
    if(not verifydataSet):
        image_dir = &quot;D:\\tmp\\cnnImage\\verify4&quot;
        verifydataSet = create_data_list(image_dir)
        print(&quot;create verifySET&quot;)

    data = verifydataSet[random.randint(0, len(verifydataSet)-1)]
    image = data[0]
    text = data[1]
    return  text,image
</code></pre><p>在监控文件所在盘 执行tensorboard –logdir=…<br>(这个cmd 命令与在百度上查询到的在linux上执行的不一样，需要注意)<br>就可以在tensorboard上查看监控图像，</p>
<p>本次训练在经过2万个step 以后ACC达到95%，识别准确率在85%</p>
<p><img src="http://chuantu.biz/t6/122/1509618243x3663627938.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因有个验证码识别的需求，本来打算用SVM来识别验证码，但在查询资料的过程中看到了@斗大的熊猫&lt;br&gt;&lt;a href=&quot;http://blog.topspeedsnail.com/archives/10858&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;T
    
    </summary>
    
      <category term="python,CNN,tensorflow," scheme="http://zhaoxj0217.github.io/categories/python-CNN-tensorflow/"/>
    
    
      <category term="深度学习" scheme="http://zhaoxj0217.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="captcha" scheme="http://zhaoxj0217.github.io/tags/captcha/"/>
    
      <category term="CNN" scheme="http://zhaoxj0217.github.io/tags/CNN/"/>
    
      <category term="tensorflow" scheme="http://zhaoxj0217.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>scikit_learn preprocessing</title>
    <link href="http://zhaoxj0217.github.io/2017/10/26/scikit_learn_preprocessing/"/>
    <id>http://zhaoxj0217.github.io/2017/10/26/scikit_learn_preprocessing/</id>
    <published>2017-10-26T07:54:57.875Z</published>
    <updated>2017-10-26T08:07:12.438Z</updated>
    
    <content type="html"><![CDATA[<p>特征数据预处理<br>参考自（<a href="http://blog.csdn.net/sinat_33761963/article/details/53433799" target="_blank" rel="external">预处理数据的方法总结（使用sklearn-preprocessing）</a> ）</p>
<ol>
<li>标准化：去均值，方差规模化</li>
</ol>
<p>Standardization标准化:将特征数据的分布调整成标准正太分布，也叫高斯分布，也就是使得数据的均值维0，方差为1.</p>
<p>标准化的原因在于如果有些特征的方差过大，则会主导目标函数从而使参数估计器无法正确地去学习其他特征。</p>
<p>标准化的过程为两步：去均值的中心化（均值变为0）；方差的规模化（方差变为1）。</p>
<p>在sklearn.preprocessing中提供了一个scale的方法，可以实现以上功能。</p>
<pre><code>from sklearn import preprocessing
import numpy as np
x = np.array([[1., -1., 2.],
  [2., 0., 0.],
  [0., 1., -1.]])
x_scale = preprocessing.scale(x)
&gt;&gt;&gt;x_scale
&gt;&gt;&gt;array([[ 0.        , -1.22474487,  1.33630621],
   [ 1.22474487,  0.        , -0.26726124],
   [-1.22474487,  1.22474487, -1.06904497]])
&gt;&gt;&gt;x_scale.mean(axis=0)
&gt;&gt;&gt;array([ 0.,  0.,  0.])
&gt;&gt;&gt;x_scale.std(axis=0)
&gt;&gt;&gt;array([ 1.,  1.,  1.])
</code></pre><p>用数学公式计算的话   </p>
<pre><code>#calculate mean
x_mean = x.mean(axis=0)
# calculate variance 
x_std = x.std(axis=0)
# standardize X
x1 = (x-x_mean)/x_std
</code></pre><p>x1的计算结果与上述使用preprocessing.scale 计算的结果一样</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;特征数据预处理&lt;br&gt;参考自（&lt;a href=&quot;http://blog.csdn.net/sinat_33761963/article/details/53433799&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;预处理数据的方法总结（使用sklearn
    
    </summary>
    
      <category term="Machine Learning,python" scheme="http://zhaoxj0217.github.io/categories/Machine-Learning-python/"/>
    
    
      <category term="Machine Learning" scheme="http://zhaoxj0217.github.io/tags/Machine-Learning/"/>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="scikit_learn" scheme="http://zhaoxj0217.github.io/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>linear algebra知识点—math</title>
    <link href="http://zhaoxj0217.github.io/2017/10/26/linear_algebra/"/>
    <id>http://zhaoxj0217.github.io/2017/10/26/linear_algebra/</id>
    <published>2017-10-26T01:39:54.128Z</published>
    <updated>2017-10-26T01:39:54.183Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="linear algebra" scheme="http://zhaoxj0217.github.io/categories/linear-algebra/"/>
    
    
      <category term="linear algebra" scheme="http://zhaoxj0217.github.io/tags/linear-algebra/"/>
    
  </entry>
  
  <entry>
    <title>NumPy—Python</title>
    <link href="http://zhaoxj0217.github.io/2017/10/25/python_numpy/"/>
    <id>http://zhaoxj0217.github.io/2017/10/25/python_numpy/</id>
    <published>2017-10-25T08:35:03.611Z</published>
    <updated>2017-10-25T08:35:03.672Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="NumPy" scheme="http://zhaoxj0217.github.io/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>python正则表达式—Python</title>
    <link href="http://zhaoxj0217.github.io/2017/10/25/python_regular/"/>
    <id>http://zhaoxj0217.github.io/2017/10/25/python_regular/</id>
    <published>2017-10-25T08:17:49.915Z</published>
    <updated>2017-10-25T08:17:49.967Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="python" scheme="http://zhaoxj0217.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://zhaoxj0217.github.io/tags/python/"/>
    
      <category term="正则表达式" scheme="http://zhaoxj0217.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
</feed>
